{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c281de67-8d5e-45be-abc0-ef96980d2774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np \n",
    "import fasttext \n",
    "import csv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from random import randrange\n",
    "import random, warnings\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "\n",
    "folder_name='xxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8311ec0d-234f-4bcf-88da-a6da5309b5fc",
     "showTitle": true,
     "title": "Helper functions for training"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper function for train_and_add_fasttext\n",
    "def get_and_join_embeddings(X_, model_embed, text_col):\n",
    "    \"\"\"\n",
    "    Output: (pandas Dataframe) \n",
    "    dataset with text embeddings added, using trained\n",
    "    fasttext model\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    X_: (pandas Dataframe) input data\n",
    "    model_embed: fasttext model\n",
    "    text_col: (string) name of text column to be embedded\n",
    "    \"\"\"\n",
    "    X_[text_col] = X_[text_col].fillna(\"\") # safeguard for NA\n",
    "    embeddings = pd.DataFrame([model_embed.get_sentence_vector(str(st)) for st in X_[text_col]]) \n",
    "    embeddings.columns = [text_col + \"_fasttext_{}\".format(i) for i in range(100)]\n",
    "    embeddings.index = X_.index # keep index intact after joining\n",
    "    X_ = X_.join(embeddings)\n",
    "    #print(X_.loc[0, [text_col, text_col + \"_fasttext_0\", text_col + \"_fasttext_1\", text_col + \"_fasttext_2\", text_col + \"_fasttext_3\"]])\n",
    "    return X_\n",
    "\n",
    "# helper function for train_and_add_fasttext\n",
    "def classify(x, th_1, th_2):\n",
    "    \"\"\"\n",
    "    Output: string\n",
    "    (label High / Med / Low corresponding to the value x,\n",
    "    using th_1 and th_2 as determining thresholds)\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    x: (float) value to be classified\n",
    "    th_1: (float) separation low / medium \n",
    "    th_2: (float) separation medium / high\n",
    "    \"\"\"\n",
    "    if x < th_1:\n",
    "        return \"__label__Low\"\n",
    "    if x < th_2:\n",
    "        return \"__label__Medium\"\n",
    "    else:\n",
    "        return \"__label__High\" \n",
    "\n",
    "\n",
    "def train_and_add_fasttext( X_train, X_valid, X_test, col_for_label, text_col):\n",
    "    \"\"\"\n",
    "    Output: (pandas Dataframe tuple)\n",
    "    X_train, X_valid, X_test with fasttext embeddings \n",
    "    trained and added\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    X_train: (pandas Dataframe) train data\n",
    "    X_valid: (pandas Dataframe) valid data\n",
    "    X_test: (pandas Dataframe) test data\n",
    "    col_for_label: (string) column used for creating labels\n",
    "    text_col: (string) name of text column to be embedded\n",
    "    \n",
    "    \"\"\"\n",
    "    # create balanced labels\n",
    "    full_data_set = pd.concat([X_train, X_valid, X_test])\n",
    "    th1 = full_data_set[col_for_label].quantile(1/3)\n",
    "    th2 = full_data_set[col_for_label].quantile(2/3)\n",
    "    \n",
    "    X_train[\"Classification_balanced\"]= X_train[col_for_label].apply(lambda x: classify(x, th1, th2)) \n",
    "    X_valid[\"Classification_balanced\"]= X_valid[col_for_label].apply(lambda x: classify(x, th1, th2)) \n",
    "    X_test[\"Classification_balanced\"]= X_test[col_for_label].apply(lambda x: classify(x, th1, th2)) \n",
    "    \n",
    "    # build dataset\n",
    "    data_train_text = pd.DataFrame(X_train[text_col]).join(X_train[\"Classification_balanced\"])\n",
    "    data_test_text = pd.DataFrame(X_test[text_col]).join(X_test[\"Classification_balanced\"])\n",
    "\n",
    "    # save set for immediate use (training), overwriting previous save\n",
    "    data_folder = \"sd_fasttext\"\n",
    "    if not os.path.isdir(r'/dbfs/mnt/'+folder_name+'/raw data/'+data_folder):\n",
    "        os.mkdir(r'/dbfs/mnt/'+folder_name+'/raw data/'+data_folder)\n",
    "\n",
    "    data_train_text.to_csv(r'/dbfs/mnt/'+folder_name+'/raw data/'+ data_folder + 'temp_text_train.txt', \n",
    "                          index = False, \n",
    "                          sep = ' ',\n",
    "                          header = None, \n",
    "                          quoting = csv.QUOTE_NONE, \n",
    "                          quotechar = \"\", \n",
    "                          escapechar = \" \")\n",
    "\n",
    "    # train fasttext model\n",
    "    model_embeddings = fasttext.train_supervised(r'/dbfs/mnt/'+folder_name+'/raw data/'+ data_folder+'temp_text_train.txt', \n",
    "                                                 wordNgrams = 2, \n",
    "                                                 epoch=10, \n",
    "                                                 dim=100)\n",
    "\n",
    "    # Include embeddings in dataset \n",
    "    X_train = get_and_join_embeddings(X_train, model_embeddings, text_col)\n",
    "    X_valid = get_and_join_embeddings(X_valid, model_embeddings, text_col)\n",
    "    X_test = get_and_join_embeddings(X_test, model_embeddings, text_col)\n",
    "    \n",
    "    return (X_train, X_valid, X_test)\n",
    "  \n",
    "def train_save_and_add_fasttext( X_train, X_valid, col_for_label, text_col, write_folder='/tmp/', output_folder='xxxx', output_suffix=''):\n",
    "    \"\"\"\n",
    "    Output: (pandas Dataframe tuple)\n",
    "    X_train, X_valid, X_test with fasttext embeddings \n",
    "    trained and added\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    X_train: (pandas Dataframe) train data\n",
    "    X_valid: (pandas Dataframe) valid data\n",
    "    X_test: (pandas Dataframe) test data\n",
    "    col_for_label: (string) column used for creating labels\n",
    "    text_col: (string) name of text column to be embedded\n",
    "    \n",
    "    \"\"\"\n",
    "    # create balanced labels\n",
    "    full_data_set = pd.concat([X_train, X_valid])\n",
    "    th1 = full_data_set[col_for_label].quantile(1/3)\n",
    "    th2 = full_data_set[col_for_label].quantile(2/3)\n",
    "    \n",
    "    X_train[\"Classification_balanced\"]= X_train[col_for_label].apply(lambda x: classify(x, th1, th2)) \n",
    "    X_valid[\"Classification_balanced\"]= X_valid[col_for_label].apply(lambda x: classify(x, th1, th2)) \n",
    "    \n",
    "    # build dataset\n",
    "    data_train_text = pd.DataFrame(X_train[text_col]).join(X_train[\"Classification_balanced\"])\n",
    "\n",
    "    # save set for immediate use (training), overwriting previous save\n",
    "    data_folder = \"sd_fasttext/\"\n",
    "    if not os.path.isdir(write_folder+data_folder):\n",
    "        os.mkdir(write_folder+data_folder)\n",
    "\n",
    "    data_train_text.to_csv(write_folder+ data_folder + 'temp_text_train.txt', \n",
    "                          index = False, \n",
    "                          sep = ' ',\n",
    "                          header = None, \n",
    "                          quoting = csv.QUOTE_NONE, \n",
    "                          quotechar = \"\", \n",
    "                          escapechar = \" \")\n",
    "\n",
    "    # train fasttext model\n",
    "    model_embeddings = fasttext.train_supervised(write_folder + data_folder+'temp_text_train.txt', \n",
    "                                                 wordNgrams = 2, \n",
    "                                                 epoch=10, \n",
    "                                                 dim=100)\n",
    "    \n",
    "    model_embeddings.save_model(write_folder + 'fasttext_model.bin')\n",
    "    copyfile(write_folder + 'fasttext_model.bin', output_folder+'fasttext_model' + output_suffix + '.bin')\n",
    "    print('Saved fasttext model in ' + output_folder+'fasttext_model' + output_suffix + '.bin')\n",
    "    \n",
    "    # Include embeddings in dataset \n",
    "    X_train = get_and_join_embeddings(X_train, model_embeddings, text_col)\n",
    "    X_valid = get_and_join_embeddings(X_valid, model_embeddings, text_col)\n",
    "    \n",
    "    return (X_train, X_valid)\n",
    "\n",
    "def train_and_add_LDA( X_train, y_train, X_valid, X_test, ncomp):\n",
    "    \"\"\"\n",
    "    Output: (array tuple)\n",
    "    X_train, X_valid, X_test with LDA trained and added\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    X_train: (array) train data\n",
    "    y_train: (1D array) training labels\n",
    "    X_valid: (array) valid data\n",
    "    X_test: (array) test data\n",
    "    ncomp: (int) nb dimensions of LDA output\n",
    "    \n",
    "    \"\"\"\n",
    "    lda = LinearDiscriminantAnalysis(n_components=ncomp)\n",
    "    X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "    X_valid_lda = lda.transform(X_valid)\n",
    "    X_test_lda = lda.transform(X_test)\n",
    "    return np.c_[X_train, X_train_lda], np.c_[X_valid, X_valid_lda], np.c_[X_test, X_test_lda]\n",
    "\n",
    "def train_save_and_add_LDA( X_train, y_train, X_valid, ncomp, write_folder='/tmp/', output_folder='xxxx', output_suffix=''):\n",
    "    \"\"\"\n",
    "    Output: (array tuple)\n",
    "    X_train, X_valid, X_test with LDA trained and added\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    X_train: (array) train data\n",
    "    y_train: (1D array) training labels\n",
    "    X_valid: (array) valid data\n",
    "    X_test: (array) test data\n",
    "    ncomp: (int) nb dimensions of LDA output\n",
    "    \n",
    "    \"\"\"\n",
    "    lda = LinearDiscriminantAnalysis(n_components=ncomp)\n",
    "    lda_model = lda.fit(X_train, y_train)\n",
    "    with open(write_folder + 'lda_model' + output_suffix + '.pkl','wb') as file:\n",
    "        pickle.dump(lda_model, file)\n",
    "    copyfile(write_folder + 'lda_model' + output_suffix + '.pkl', output_folder + 'lda_model' + output_suffix + '.pkl')\n",
    "    print('Saved LDA model in ' + output_folder + 'lda_model' + output_suffix + '.pkl')\n",
    "    with open(write_folder + 'lda_model' + output_suffix + '.pkl','rb') as file:\n",
    "        lda_model = pickle.load(file)\n",
    "    X_train_lda = lda_model.transform(X_train)\n",
    "    X_valid_lda = lda_model.transform(X_valid)\n",
    "    return np.c_[X_train, X_train_lda], np.c_[X_valid, X_valid_lda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "946df527-7e7b-461a-9074-dd1998c6eea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_random_train_test(index_length, train_pct=0.75, seed = 1):\n",
    "    random.seed(seed)\n",
    "    ind_train = random.sample( range(index_length), int(np.floor(train_pct * index_length)) ) \n",
    "    ind_train.sort()\n",
    "    mask=np.full(index_length,True,dtype=bool)\n",
    "    mask[ind_train] = False\n",
    "    ind_test = np.array(range(index_length))[mask].tolist()\n",
    "    return [ind_train, ind_test]\n",
    "    \n",
    "def split_random_train_test_valid(index_length, train_pct=0.75, test_pct = 0.15, seed = 1):\n",
    "    random.seed(seed)\n",
    "    ind_train = random.sample( range(index_length), int(np.floor(train_pct * index_length)) ) \n",
    "    ind_train.sort()\n",
    "    mask=np.full(index_length,True,dtype=bool)\n",
    "    mask[ind_train] = False\n",
    "    ind_test = np.array(range(index_length))[mask].tolist()    \n",
    "    ind_test = random.sample(ind_test, int(np.floor(test_pct *index_length)) )\n",
    "    mask[ind_test] = False\n",
    "    ind_valid = np.array(range(index_length))[mask].tolist() \n",
    "    return [ind_train, ind_test, ind_valid]\n",
    "\n",
    "def prepare_data_for_training(path_to_data = folder_name + 'data_prepped.csv',\n",
    "                                                              method = 'random', # no alternative implemented here, but could be chronological time series splits\n",
    "                                                              seed = 1,\n",
    "                                                              column_for_classification = 'Gross_Incurred_Detrended',\n",
    "                                                              quantile_high = 0.90,\n",
    "                                                              quantile_medium = 0.80,\n",
    "                                                              proportion_train = 0.80,\n",
    "                                                              proportion_test = 0.15,\n",
    "                                                              cutoff_claim_loss_date = 2020, # Claims with Claim_Loss_Date from this point onwards are discarded,\n",
    "                                                              path_to_output_train = folder_name + 'data_train.csv',\n",
    "                                                              path_to_output_valid = folder_name + 'data_valid.csv',\n",
    "                                                              path_to_output_test = folder_name + 'data_test.csv'\n",
    "                                                             ):\n",
    "    \"\"\"\n",
    "    Output: (array tuple)\n",
    "    data_train, data_valid, data_test: DataFrames that together equal the total data set (with adjustments such as removing undevelopped claims, sorting, and labels added)\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    path_to_date: (string) path to csv file with data (required to be clean first using prepare_dataset())\n",
    "    method: (string) currently always 'random', in the future alternative splits such as time series splits can be implemented here\n",
    "    seed: (int) seed to be used for randomly dividing data into train, valid and test\n",
    "    quantile_high: (float) quantile for high claims (default: 0.90, i.e. only top 10% of claims are high)\n",
    "    quantile_medium: (float) quantile for medium claims (default: 0.80, i.e. 80% of claims are lower than medium)\n",
    "    proportion_train: (float) proportion of data that is used for training\n",
    "    proportion_test: (float) proportion of data that is used for testing. Note that 1 - proportion_train - proportion_test will be used for validation\n",
    "    cutoff_claim_loss_date: (string) cut-off date after which claims are discarded for training due to not being sufficiently developed\n",
    "    path_to_output_train: (string) path to where the train data set is saved as csv\n",
    "    path_to_output_valid: (string) path to where the valid data set is saved as csv\n",
    "    path_to_output_test: (string) path to where the test data set is saved as csv\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path_to_data, low_memory=False) # read data\n",
    "    data = data[data[\"Claim_Loss_Date\"]<str(cutoff_claim_loss_date)] # cut-off claims that aren't sufficiently  developed\n",
    "    data = data.sort_values(by=\"Claim_Loss_Date\").reset_index(drop=True) # sort by Claim_Loss_Date\n",
    "   \n",
    "    # Calculate thresholds and set levels for the given high and medium quantile\n",
    "    high_threshold = data[column_for_classification].quantile(quantile_high) ### Threshold between medium and high <= change here\n",
    "    medium_threshold = data[column_for_classification].quantile(quantile_medium) ### Threshold between low and medium <= change here\n",
    "    data['Classification'] = 'Low'\n",
    "    data.loc[medium_threshold < data[column_for_classification],'Classification'] = 'Medium'\n",
    "    data.loc[high_threshold < data[column_for_classification],'Classification'] = 'High'\n",
    "    \n",
    "    # Classification for training (binary, high vs non-high, 1 or 0)\n",
    "    data['Classification_for_training'] = 0\n",
    "    data.loc[high_threshold < data[column_for_classification],'Classification_for_training'] = 1\n",
    "    \n",
    "    # Split data into train, valid and test\n",
    "    index_length = len(data)\n",
    "    random.seed(seed)    # set seed so the same split can be reproduced\n",
    "    ind_train = random.sample( range(index_length), int(np.floor(proportion_train * index_length)) ) \n",
    "    ind_train.sort()\n",
    "    mask=np.full(index_length,True,dtype=bool)\n",
    "    mask[ind_train] = False\n",
    "    ind_test = np.array(range(index_length))[mask].tolist()    \n",
    "    ind_test = random.sample(ind_test, int(np.floor(proportion_test *index_length)) )\n",
    "    mask[ind_test] = False\n",
    "    ind_valid = np.array(range(index_length))[mask].tolist() \n",
    "    data_train, data_valid, data_test = data.reindex(ind_train),data.reindex(ind_valid), data.reindex(ind_test)\n",
    "    data_train.to_csv(path_to_output_train, index=False)\n",
    "    data_valid.to_csv(path_to_output_valid, index=False)\n",
    "    data_test.to_csv(path_to_output_test, index=False)\n",
    "    return data_train, data_valid, data_test\n",
    "\n",
    "\n",
    "def set_model_settings(lgbm_params = {'num_leaves': 31, 'objective': 'binary', 'boosting':'dart', 'num_iterations':1000, 'verbose': 0 },\n",
    "                                    path_to_model_columns = folder_name + 'Model_v14.08.2022/model_columns.csv',\n",
    "                                    add_Fasttext = True,\n",
    "                                    add_LDA = True,\n",
    "                                    path_to_output = folder_name + 'Model_v14.08.2022/model_settings.pkl',\n",
    "                                    write_folder = '/tmp/'\n",
    "                                   ):\n",
    "    \"\"\"\n",
    "    Updates and saves settings for LGBM model\n",
    "    Output: (dictionary) dictionary of model settings\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    lgbm_params: (dictionary) dictionary with lgbm training settings\n",
    "    path_to_model_columns: (string) csv file with columns to include for training, as well as which columns are to be treated as categorical\n",
    "    add_Fasttext: (bool) include Fasttext embedding in model or not\n",
    "    add_LDA: (bool) include LDA in model or not\n",
    "    path_to_output: (string) pkl file where model settings will be stored\n",
    "    write_folder: (string) folder where intermediate steps can temporariliy be written to (necessary due to Databricks restrictions)\n",
    "    \"\"\"\n",
    "    columns_df = pd.read_csv(path_to_model_columns) # read csv file with which columns to include\n",
    "    model_columns  = columns_df[\"columns\"].to_list()\n",
    "    cat_columns = columns_df[columns_df[\"is_cat_feature\"]==True][\"columns\"].to_list() # get which columns are to be treated as categorical\n",
    "    cat_feat_encoding_python = [model_columns.index(c) for c in cat_columns] \n",
    "    lgbm_params['categorical_feature']=cat_feat_encoding_python \n",
    "    model_settings = {'lgbm_params':lgbm_params, 'model_columns': model_columns, 'add_Fasttext':add_Fasttext, 'add_LDA':add_LDA} # model settings are combined here\n",
    "    with open(write_folder + 'model_settings.pkl','wb') as file:\n",
    "        pickle.dump(model_settings, file) # save model settings to temporary folder\n",
    "    copyfile(write_folder + 'model_settings.pkl', path_to_output) # copy model settings to final folder\n",
    "    print()\n",
    "    print('model settings saved in ' + path_to_output )  \n",
    "    print(model_settings)\n",
    "    print()\n",
    "    return model_settings\n",
    "\n",
    "\n",
    "def train_lgbm_binary(data_train,  data_valid,\n",
    "                    lgb_params, \n",
    "                    cols, \n",
    "                    text_col_to_embed=\"concatenated_text\", \n",
    "                    add_LDA=False,\n",
    "                    add_Fasttext=True,\n",
    "                    col_for_label=\"Gross_Incurred_Detrended\",\n",
    "                    col_with_label=\"Classification_for_training\",\n",
    "                    time_col_name=\"Claim_Loss_Date\",\n",
    "                    write_folder='/tmp/', \n",
    "                    output_folder='xxxx', \n",
    "                    output_suffix=''):\n",
    "    \"\"\"\n",
    "    Output: (LGBM.Booster) Fitted LGBM Model\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    data: (pandas Dataframe) full dataset  \n",
    "    lgb_params: (dict) parameters for the LGBM classifier\n",
    "    cols: (string list) columns to use for training the model\n",
    "    text_col_to_embed: (string) column to embed in fasttext\n",
    "    add_LDA: (bool) if True, add LDA column before training LGBM\n",
    "    add_Fasttext: (bool) if True, add fasttext embedding before training LGBM\n",
    "    col_for_label: (string) column used for generating labels\n",
    "    col_with_label: (string) column with the current (actual) labels\n",
    "    time_col_name: (string) column used to sort timeseries\n",
    "    \"\"\"\n",
    "    print(\"Training model with training set of size \" + str(data_train.shape[0]) + \" and validation set of size \" + str(data_valid.shape[0]) )\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    # Select columns\n",
    "    y_train, y_valid = data_train[col_with_label], data_valid[col_with_label]\n",
    "    cols_full = cols + [text_col_to_embed, col_for_label]\n",
    "    X_train, X_valid = data_train[cols_full], data_valid[cols_full]\n",
    "    #print(X_train.dtypes)\n",
    "    \n",
    "    # add fasttext embeddings\n",
    "    if add_Fasttext: \n",
    "        print()\n",
    "        print(\"Training Fasttext embedding...\")\n",
    "        X_train, X_valid = train_save_and_add_fasttext(X_train, X_valid, col_for_label=col_for_label, text_col=text_col_to_embed, write_folder = write_folder, output_folder = output_folder, output_suffix = output_suffix) \n",
    "        X_train = X_train.drop(columns=\"Classification_balanced\")\n",
    "        X_valid = X_valid.drop(columns=\"Classification_balanced\")\n",
    "    X_train = X_train.drop(columns=[text_col_to_embed, col_for_label])\n",
    "    X_valid = X_valid.drop(columns=[text_col_to_embed, col_for_label])\n",
    "    \n",
    "    # Make array\n",
    "    y_train, y_valid = y_train.to_numpy(), y_valid.to_numpy()    \n",
    "    X_train, X_valid = X_train.to_numpy(), X_valid.to_numpy()\n",
    " \n",
    "    # add LDA features\n",
    "    if add_LDA: \n",
    "        print()\n",
    "        print(\"Training Linear Discriminant Analysis...\")\n",
    "        X_train, X_valid = train_save_and_add_LDA( X_train,y_train, X_valid, ncomp=1, write_folder = write_folder, output_folder = output_folder, output_suffix = output_suffix) \n",
    "        \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Light GBM model\n",
    "    bst = lgb.LGBMClassifier(**lgb_params)   # Initialise lgbm\n",
    "    print()\n",
    "    print(\"Training lgbm...\")\n",
    "    callbacks = [lgb.early_stopping(10, verbose=-1), lgb.log_evaluation(period=100)] # Settings for printing output during training\n",
    "    bst.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=callbacks) # Actual training\n",
    "    \n",
    "    # Save results\n",
    "    with open(write_folder + 'lgbm_model' + output_suffix + '.pkl','wb') as file:\n",
    "        pickle.dump(bst, file)\n",
    "    copyfile(write_folder + 'lgbm_model' + output_suffix + '.pkl', output_folder + 'lgbm_model' + output_suffix + '.pkl')\n",
    "    print()\n",
    "    print('lgbm model saved in ' + output_folder + 'lgbm_model' + output_suffix + '.pkl' )        \n",
    "    return bst\n",
    "\n",
    "def test_lgbm_binary(data_test,\n",
    "                    cols, \n",
    "                    text_col_to_embed=\"concatenated_text\", \n",
    "                    add_Fasttext=True,\n",
    "                    add_LDA=False,\n",
    "                    output_folder='xxxx', \n",
    "                    output_suffix='',\n",
    "                    col_predicted_prob = 'predicted_prob'\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Output: (Dataframe) Dataframe with added predicted probabilities high vs non-high\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    data_test: (pandas Dataframe) full dataset      \n",
    "    cols: (string list) columns to use for training the model\n",
    "    text_col_to_embed: (string) column to embed in fasttext\n",
    "    add_LDA: (bool) if True, add LDA column before training LGBM\n",
    "    add_Fasttext: (bool) if True, add fasttext embedding before training LGBM    \n",
    "    \"\"\"\n",
    "    X_test = data_test[cols + [text_col_to_embed]] # remove unnecessary columns\n",
    "    if add_Fasttext:        \n",
    "        fasttext_model = fasttext.load_model(output_folder + 'fasttext_model' +output_suffix + '.bin')\n",
    "        X_test = get_and_join_embeddings(X_test, fasttext_model, text_col_to_embed)         \n",
    "    X_test = X_test.drop(columns=text_col_to_embed)\n",
    "    #print(X_test.dtypes)\n",
    "    X_test = X_test.to_numpy()\n",
    "    if add_LDA:\n",
    "        with open(output_folder + 'lda_model' + output_suffix + '.pkl','rb') as file:\n",
    "            lda_model = pickle.load(file)\n",
    "            X_test = np.c_[X_test, lda_model.transform(X_test)] \n",
    "    with open(output_folder + 'lgbm_model' + output_suffix + '.pkl','rb') as file:\n",
    "        lgb_model = pickle.load(file)\n",
    "    prob_test = lgb_model.predict_proba(X_test)\n",
    "    data_test[col_predicted_prob] = prob_test[:,1]\n",
    "    return data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ccc1e0f-5a09-4835-ade0-672e696815ab",
     "showTitle": true,
     "title": "Train light GBM"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "def train_model(path_to_data_train = folder_name + 'data_train.csv',\n",
    "                path_to_data_valid = folder_name + 'data_valid.csv',\n",
    "                path_to_model_settings = folder_name + 'model_settings.pkl', \n",
    "                output_folder = folder_name,\n",
    "                output_suffix = \"_v14.08.2022\",\n",
    "                col_for_label='Gross_Incurred_Detrended'\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Trains a binary LGBM classifier using cross validation\n",
    "    Output: (LGBM.Booster) Trained model\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    path_to_data_train: (string) csv file with training data  \n",
    "    path_to_data_valid: (string) csv file with validation data\n",
    "    path_to_model_settings: (string) pkl file with model settings\n",
    "    output_folder: (string) folder where model files will be stored\n",
    "    output_suffix: (string) (optional) suffix to be added to model file names\n",
    "    \"\"\"\n",
    "    data_train = pd.read_csv(path_to_data_train, low_memory=False) # reads data set used for training\n",
    "    data_valid = pd.read_csv(path_to_data_valid, low_memory=False) # reads data set used for validation\n",
    "\n",
    "    with open(path_to_model_settings,'rb') as file:\n",
    "        model_settings = pickle.load(file) # loads model settings\n",
    "    \n",
    "    lgb_params = model_settings['lgbm_params']\n",
    "    bst = train_lgbm_binary(data_train = data_train,  \n",
    "                               data_valid=data_valid, \n",
    "                               lgb_params = model_settings['lgbm_params'], \n",
    "                               cols = model_settings['model_columns'], \n",
    "                               text_col_to_embed='concatenated_text', \n",
    "                               add_Fasttext=model_settings['add_Fasttext'], \n",
    "                               add_LDA=model_settings['add_LDA'], \n",
    "                               col_for_label=col_for_label, \n",
    "                               col_with_label = 'Classification_for_training', \n",
    "                               time_col_name='Claim_Loss_Date', \n",
    "                               write_folder='/tmp/', # temporary writing folder because Databricks has restrictions on writing to files\n",
    "                               output_folder=output_folder, \n",
    "                               output_suffix=output_suffix)\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc09bf1-8e7d-44db-a4fd-c0bf54a4982e",
     "showTitle": true,
     "title": "Load and apply light GBM"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_apply_lgbm(path_to_data = folder_name + 'unlabelled_data_prepped.csv',\n",
    "                        model_folder = '/dbfs/mnt/'+folder_name+'/clean_gbm/',\n",
    "                        model_suffix = \"v14.08.2022\",\n",
    "                        path_to_model_settings = folder_name + 'model_settings.pkl', \n",
    "                        path_to_output = folder_name + 'unlabelled_data_predicted_probs.csv',\n",
    "                        col_predicted_prob = 'predicted_prob'\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    Applies a trained binary LGBM classifier to a given data set\n",
    "    Output: (pandas DataFrame) data set with a columns with predicted probabilities added\n",
    "    _____________________________________________________\n",
    "    Parameters\n",
    "    path_to_data: (string) csv file with data for which probabilities will be predicted  \n",
    "    model_folder: (string) folder where model files are stored\n",
    "    model_suffix: (string) suffix that was added to model files\n",
    "    path_to_model_settings: (string) pkl file with model settings\n",
    "    path_to_output: (string) csv file where data set with added predicted probabilities will be stored\n",
    "    \"\"\"\n",
    "    data_test = pd.read_csv(path_to_data, low_memory=False) # read data to apply model to\n",
    "\n",
    "    with open(path_to_model_settings,'rb') as file:\n",
    "        model_settings = pickle.load(file) # load model settings\n",
    "    \n",
    "    data_tested = test_lgbm_binary(data_test = data_test, \n",
    "                                      cols = model_settings['model_columns'], \n",
    "                                      text_col_to_embed=\"concatenated_text\", \n",
    "                                      add_Fasttext=model_settings['add_Fasttext'], # Currently turned off\n",
    "                                      add_LDA=model_settings['add_LDA'], \n",
    "                                      output_folder=model_folder, \n",
    "                                      output_suffix=model_suffix,\n",
    "                                      col_predicted_prob=col_predicted_prob\n",
    "                                  )\n",
    "    \n",
    "    data_tested.to_csv(path_to_output, index=False) # save data set with added probabilities\n",
    "    return data_tested\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58dce9d6-ce24-4f93-8d36-28e37c38a0ae",
     "showTitle": true,
     "title": "Performance measures and diagnostics"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tp_tn_fp_fn(predicted, actual):\n",
    "    \"\"\"\n",
    "    Output: (int tuple)\n",
    "    for binary classification evaluation\n",
    "    number of true positives, true negatives, false positives, \n",
    "    false negatives \n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    predicted: (int list) list of predicted classes\n",
    "    actual: (int list) list of labels\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = 0, 0 , 0 , 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == 0:\n",
    "            if predicted[i] == actual[i]:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted[i] == actual[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def precision(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Output: (float)\n",
    "    precision based on true / false +ve, true / false -ve\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    tp, tn, fp, fn : (all int) number of true positives, \n",
    "                    true negatives, false positives, false \n",
    "                    negatives \n",
    "    \"\"\"\n",
    "    return tp/(tp + fp)\n",
    "\n",
    "def recall(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Output: (float)\n",
    "    recall based on true / false +ve, true / false -ve\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    tp, tn, fp, fn : (all int) number of true positives, \n",
    "                    true negatives, false positives, false \n",
    "                    negatives \n",
    "    \"\"\"\n",
    "    return tp/(tp + fn)\n",
    "\n",
    "def total_predict(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Output: (float)\n",
    "    proportion predicted 1 based on true / false +ve and\n",
    "    true / false -ve\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    tp, tn, fp, fn : (all int) number of true positives, \n",
    "                    true negatives, false positives, false \n",
    "                    negatives \n",
    "    \"\"\"\n",
    "    return (tp + fp)/(tp+tn+fp+fn)\n",
    "\n",
    "# helper functions for prediction \n",
    "\n",
    "def predict(y, level, ind):\n",
    "    \"\"\"\n",
    "    Output: (int list)\n",
    "    list of int corresponding to prediction 1 or 0 for a class\n",
    "    indicated by ind\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    y: (iterable) sequence of probability class predictions\n",
    "    level: (float) between (0, 1) - probability threshold \n",
    "            to predict 1\n",
    "    ind: (int) if 0: y is a list of probabilities\n",
    "               else: y is a list of vectors of probabilities,\n",
    "                     and ind gives the index of the class we \n",
    "                     want to predict\n",
    "    \"\"\"\n",
    "    if ind == 0: \n",
    "        return [int(u>level) for u in y]\n",
    "    return [int(u[ind]>level) for u in y]\n",
    "\n",
    "\n",
    "# helper function for find_thresh\n",
    "def calc_total_predict(pred_weight, actual, level):\n",
    "    \"\"\"\n",
    "    Output: (float)\n",
    "    amount predicted 1 based on 1 dimensional vector of \n",
    "    class prediction probabilities, labels, and probability\n",
    "    level\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    pred_weight: (float list) prediction probabilities of class 1 \n",
    "    actual: (int list) list of labels\n",
    "    level: (float) between (0, 1) - probability threshold \n",
    "            to predict 1\n",
    "    \"\"\"\n",
    "    pred = predict(pred_weight, level, 0)\n",
    "    tp, tn, fp, fn = tp_tn_fp_fn(pred, actual)\n",
    "    return total_predict(tp, tn, fp, fn)\n",
    "\n",
    "def find_thresh(pred_weight, actual, level_a, level_b, predicted=0.1, error=0.001):\n",
    "    \"\"\"\n",
    "    Output: (float)\n",
    "    probability threshold level needed to reach an amount \n",
    "    of 1 prediction (within error on amount)\n",
    "    (obtained by dichotomy search)\n",
    "    _____________________________________________________\n",
    "    Parameters:\n",
    "    pred_weight: (float list) prediction probabilities of class 1 \n",
    "    actual: (int list) list of labels\n",
    "    level_a: (float) between (0, 1) - probability threshold lower bound\n",
    "    level_a: (float) between (0, 1) - probability threshold upper bound\n",
    "    predicted: (float) amount of prediction 1 aimed\n",
    "    error: (float) absolute error allowed on amount of prediction 1        \n",
    "    \"\"\"\n",
    "    pos_a = calc_total_predict(pred_weight, actual, level_a) \n",
    "    pos_b = calc_total_predict(pred_weight, actual, level_b)\n",
    "    \n",
    "    if abs(pos_a - predicted) < error :\n",
    "        return level_a, pos_a\n",
    "    elif abs(pos_b - predicted) < error :\n",
    "        return level_b, pos_b\n",
    "    \n",
    "    level_c = (level_a + level_b)/2\n",
    "    pos_c = calc_total_predict(pred_weight, actual, level_c)\n",
    "    \n",
    "    if abs(pos_c - predicted) < error :\n",
    "        return level_c, pos_c\n",
    "    elif pos_c > predicted:\n",
    "        return find_thresh(pred_weight, actual, level_c, level_b, predicted=predicted)\n",
    "    elif pos_c < predicted:\n",
    "        return find_thresh(pred_weight, actual, level_a, level_c, predicted=predicted)\n",
    "    return\n",
    "\n",
    "def get_probability_threshold(data, col_predicted_prob='predicted_prob', target_pct = 0.10):\n",
    "    data = data.sort_values(col_predicted_prob).reset_index(drop=True)\n",
    "    return data.loc[int((1-target_pct)*data.shape[0])][col_predicted_prob]\n",
    "\n",
    "def get_probability_thresholds(data, col_predicted_prob='predicted_prob', quantiles = [0.10, 0.20]):\n",
    "    data = data.sort_values(col_predicted_prob).reset_index(drop=True)\n",
    "    return [data.loc[int((1-q)*data.shape[0])][col_predicted_prob] for q in quantiles]\n",
    "\n",
    "\n",
    "def predict_labels_from_probs(path_to_data = 'unlabelled_data_predicted_probs', \n",
    "                              probability_thresholds = [0.3], # when list give in \n",
    "                              col_predicted_prob='predicted_prob',\n",
    "                              col_predicted_label='predicted_label',\n",
    "                              path_to_output = folder_name + 'data_predicted_labels.csv'):\n",
    "    data = pd.read_csv(path_to_data, low_memory=False)\n",
    "    probability_thresholds.sort()\n",
    "    data[col_predicted_label] = 0\n",
    "    for i in range(len(probability_thresholds)):\n",
    "        data[col_predicted_label] = data[col_predicted_label] + data[col_predicted_prob].apply(lambda x: int(x>probability_thresholds[i]))\n",
    "    data.to_csv(path_to_output,index=False)\n",
    "    return data\n",
    "\n",
    "def print_performance_metrics(path_to_data = folder_name + 'data_predicted_labels.csv',\n",
    "                         col_actual_labels = 'Classification_for_training',\n",
    "                         col_predicted_label = 'predicted_label',\n",
    "                         col_predicted_probability = 'predicted_prob'):\n",
    "    data = pd.read_csv(path_to_data, low_memory=False)\n",
    "    TP, TN, FP, FN = tp_tn_fp_fn(data[col_predicted_label], data[col_actual_labels]) # TRUE/FALSE POSITIVE/NEGATIVE\n",
    "    Recall = recall(TP, TN, FP, FN)\n",
    "    Precision = precision(TP, TN, FP, FN)\n",
    "    Ratio_high = total_predict(TP, TN, FP, FN)\n",
    "    AUC = roc_auc_score(data[col_actual_labels],data[col_predicted_probability])\n",
    "    performance_metrics = {'AUC':AUC,\n",
    "           'recall':Recall,\n",
    "           'precision':Precision,\n",
    "           'ratio_high':Ratio_high,\n",
    "            'TP':TP,\n",
    "            'TN':TN,\n",
    "            'FP':FP,\n",
    "            'FN':FN\n",
    "           }\n",
    "    # print the results\n",
    "    print()\n",
    "    print(\"Test performance measures:\")\n",
    "    print(performance_metrics) \n",
    "    return performance_metrics\n",
    "    \n",
    "def print_model_diagnostics(model, model_settings):\n",
    "    print()\n",
    "    print(\"Model has \" + str(model.n_features_) + \" features.\")\n",
    "    \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    y = trained_model.feature_importances_\n",
    "    #x = pd.read_csv(path_to_model_columns)[\"columns\"].to_list()\n",
    "    x = model_settings['model_columns']\n",
    "    if model_settings['add_Fasttext']:\n",
    "        x = x + [\"fasttext_{}\".format(i) for i in range(100)]\n",
    "    if model_settings['add_LDA']:   \n",
    "        x = x + [\"LDA\"] \n",
    "    fig = plt.figure(figsize=(8, len(x)/4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.barh(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1abbcfb2-0fe7-45f6-8e88-7473c8458427",
     "showTitle": true,
     "title": "Update which columns to include in the model - change columns, uncomment and run this cell to update the included columns"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;Worker_Gender&#39;, &#39;Worker_Marital_Status&#39;, &#39;Risk_State&#39;, &#39;Full/Part_Time_Flag&#39;, &#39;Injury_Mechanism_Grouped&#39;, &#39;Body_Location_Grouped&#39;, &#39;Industry&#39;, &#39;Occupation_Name_Grouped&#39;, &#39;Liability_Status_Grouped&#39;, &#39;Weekly_Rate_Known&#39;, &#39;Work_Resumed_Within_30days_Flag&#39;, &#39;Duty_Status&#39;, &#39;strain&#39;, &#39;laceration&#39;, &#39;injury&#39;, &#39;body&#39;, &#39;sprain&#39;, &#39;pain&#39;, &#39;fracture&#39;, &#39;contusion&#39;, &#39;burn&#39;, &#39;crush&#39;, &#39;bruise&#39;, &#39;abrasion&#39;, &#39;tear&#39;, &#39;cut&#39;, &#39;wound&#39;, &#39;bite&#39;, &#39;anxiety&#39;, &#39;broken&#39;, &#39;puncture&#39;, &#39;shock&#39;, &#39;whiplash&#39;, &#39;fall&#39;, &#39;amputation&#39;, &#39;SEIFA&#39;]\n",
       "[False, False, False, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
       "[&#39;Worker_Age&#39;, &#39;Weekly_Rate_First13_Indexed&#39;, &#39;Claim_Loss_Date_Num&#39;, &#39;Worker_Gender&#39;, &#39;Worker_Marital_Status&#39;, &#39;Risk_State&#39;, &#39;Full/Part_Time_Flag&#39;, &#39;Policy_Duration&#39;, &#39;Claim_Notified_Delay&#39;, &#39;Liability_Delay&#39;, &#39;strain&#39;, &#39;laceration&#39;, &#39;injury&#39;, &#39;body&#39;, &#39;sprain&#39;, &#39;pain&#39;, &#39;fracture&#39;, &#39;contusion&#39;, &#39;burn&#39;, &#39;crush&#39;, &#39;bruise&#39;, &#39;abrasion&#39;, &#39;tear&#39;, &#39;cut&#39;, &#39;wound&#39;, &#39;bite&#39;, &#39;anxiety&#39;, &#39;broken&#39;, &#39;puncture&#39;, &#39;shock&#39;, &#39;whiplash&#39;, &#39;fall&#39;, &#39;amputation&#39;, &#39;Injury_Mechanism_Grouped&#39;, &#39;Body_Location_Grouped&#39;, &#39;Industry&#39;, &#39;Occupation_Name_Grouped&#39;, &#39;Liability_Status_Grouped&#39;, &#39;Weekly_Rate_Known&#39;, &#39;Work_Resumed_Within_30days_Flag&#39;, &#39;Duty_Status&#39;, &#39;SEIFA&#39;, &#39;Time_Since_Orig_Policy_Inception&#39;, &#39;incurred_t30_indexed&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;Worker_Gender&#39;, &#39;Worker_Marital_Status&#39;, &#39;Risk_State&#39;, &#39;Full/Part_Time_Flag&#39;, &#39;Injury_Mechanism_Grouped&#39;, &#39;Body_Location_Grouped&#39;, &#39;Industry&#39;, &#39;Occupation_Name_Grouped&#39;, &#39;Liability_Status_Grouped&#39;, &#39;Weekly_Rate_Known&#39;, &#39;Work_Resumed_Within_30days_Flag&#39;, &#39;Duty_Status&#39;, &#39;strain&#39;, &#39;laceration&#39;, &#39;injury&#39;, &#39;body&#39;, &#39;sprain&#39;, &#39;pain&#39;, &#39;fracture&#39;, &#39;contusion&#39;, &#39;burn&#39;, &#39;crush&#39;, &#39;bruise&#39;, &#39;abrasion&#39;, &#39;tear&#39;, &#39;cut&#39;, &#39;wound&#39;, &#39;bite&#39;, &#39;anxiety&#39;, &#39;broken&#39;, &#39;puncture&#39;, &#39;shock&#39;, &#39;whiplash&#39;, &#39;fall&#39;, &#39;amputation&#39;, &#39;SEIFA&#39;]\n[False, False, False, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n[&#39;Worker_Age&#39;, &#39;Weekly_Rate_First13_Indexed&#39;, &#39;Claim_Loss_Date_Num&#39;, &#39;Worker_Gender&#39;, &#39;Worker_Marital_Status&#39;, &#39;Risk_State&#39;, &#39;Full/Part_Time_Flag&#39;, &#39;Policy_Duration&#39;, &#39;Claim_Notified_Delay&#39;, &#39;Liability_Delay&#39;, &#39;strain&#39;, &#39;laceration&#39;, &#39;injury&#39;, &#39;body&#39;, &#39;sprain&#39;, &#39;pain&#39;, &#39;fracture&#39;, &#39;contusion&#39;, &#39;burn&#39;, &#39;crush&#39;, &#39;bruise&#39;, &#39;abrasion&#39;, &#39;tear&#39;, &#39;cut&#39;, &#39;wound&#39;, &#39;bite&#39;, &#39;anxiety&#39;, &#39;broken&#39;, &#39;puncture&#39;, &#39;shock&#39;, &#39;whiplash&#39;, &#39;fall&#39;, &#39;amputation&#39;, &#39;Injury_Mechanism_Grouped&#39;, &#39;Body_Location_Grouped&#39;, &#39;Industry&#39;, &#39;Occupation_Name_Grouped&#39;, &#39;Liability_Status_Grouped&#39;, &#39;Weekly_Rate_Known&#39;, &#39;Work_Resumed_Within_30days_Flag&#39;, &#39;Duty_Status&#39;, &#39;SEIFA&#39;, &#39;Time_Since_Orig_Policy_Inception&#39;, &#39;incurred_t30_indexed&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_columns_to_include(path_to_output = folder_name + 'model_columns.csv'):\n",
    "    columns = [\n",
    "                                            \"Worker_Age\", \n",
    "                                            \"Weekly_Rate_First13_Indexed\",  \n",
    "                                            \"Claim_Loss_Date_Num\", \n",
    "                                            \"Worker_Gender\",\n",
    "                                            \"Worker_Marital_Status\",\n",
    "                                            \"Risk_State\",\n",
    "                                            \"Full/Part_Time_Flag\",\n",
    "                                            \"Policy_Duration\",\n",
    "                                            \"Claim_Notified_Delay\", \n",
    "                                            \"Liability_Delay\",\n",
    "                                            \"strain\",\n",
    "                                            \"laceration\",\n",
    "                                            \"injury\",\n",
    "                                            \"body\",\n",
    "                                            \"sprain\",\n",
    "                                            \"pain\",\n",
    "                                            \"fracture\",\n",
    "                                            \"contusion\",\n",
    "                                            \"burn\",\n",
    "                                            \"crush\",\n",
    "                                            \"bruise\",\n",
    "                                            \"abrasion\",\n",
    "                                            \"tear\",\n",
    "                                            \"cut\",\n",
    "                                            \"wound\",\n",
    "                                            \"bite\",\n",
    "                                            \"anxiety\",\n",
    "                                            \"broken\",\n",
    "                                            \"puncture\",\n",
    "                                            \"shock\",\n",
    "                                            \"whiplash\",\n",
    "                                            \"fall\",\n",
    "                                            \"amputation\", \n",
    "                                            \"Injury_Mechanism_Grouped\",\n",
    "                                            \"Body_Location_Grouped\", \n",
    "                                            \"Industry\", \n",
    "                                            \"Occupation_Name_Grouped\", \n",
    "                                            \"Liability_Status_Grouped\", \n",
    "                                            \"Weekly_Rate_Known\",\n",
    "                                            \"Work_Resumed_Within_30days_Flag\", \n",
    "                                            \"Duty_Status\", \n",
    "                                            \"SEIFA\", \n",
    "                                            \"Time_Since_Orig_Policy_Inception\", \n",
    "                                            \"incurred_t30_indexed\"]\n",
    "    cat_columns = [\n",
    "                                            \"Worker_Gender\",\n",
    "                                            \"Worker_Marital_Status\",\n",
    "                                            \"Risk_State\", \n",
    "                                            \"Full/Part_Time_Flag\",\n",
    "                                            \"Injury_Mechanism_Grouped\",\n",
    "                                            \"Body_Location_Grouped\", \n",
    "                                            \"Industry\", \n",
    "                                            \"Occupation_Name_Grouped\", \n",
    "                                            \"Liability_Status_Grouped\",\n",
    "                                            \"Weekly_Rate_Known\",\n",
    "                                            \"Work_Resumed_Within_30days_Flag\", \n",
    "                                            \"Duty_Status\",\n",
    "                                            \"strain\",\n",
    "                                            \"laceration\",\n",
    "                                            \"injury\",\n",
    "                                            \"body\",\n",
    "                                            \"sprain\",\n",
    "                                            \"pain\",\n",
    "                                            \"fracture\",\n",
    "                                            \"contusion\",\n",
    "                                            \"burn\",\n",
    "                                            \"crush\",\n",
    "                                            \"bruise\",\n",
    "                                            \"abrasion\",\n",
    "                                            \"tear\",\n",
    "                                            \"cut\",\n",
    "                                            \"wound\",\n",
    "                                            \"bite\",\n",
    "                                            \"anxiety\",\n",
    "                                            \"broken\",\n",
    "                                            \"puncture\",\n",
    "                                            \"shock\",\n",
    "                                            \"whiplash\",\n",
    "                                            \"fall\",\n",
    "                                            \"amputation\", \n",
    "                                            \"SEIFA\"]\n",
    "    is_cat_feature = [(c in cat_columns) for c in columns]\n",
    "    print(cat_columns)\n",
    "    print(is_cat_feature)\n",
    "    cols_to_include = pd.DataFrame({'columns': columns, 'is_cat_feature': is_cat_feature})\n",
    "    cols_to_include.to_csv(path_to_output, index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "# Test\n",
    "#update_columns_to_include('/dbfs/mnt/augiprojects-20220225workerscomp/for sharing/Model_v14.08.2022/model_columns.csv')\n",
    "#model_columns  = pd.read_csv('/dbfs/mnt/augiprojects-20220225workerscomp/for sharing/Model_v14.08.2022/model_columns.csv')[\"columns\"].to_list()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions_model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
