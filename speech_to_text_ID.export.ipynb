{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82533663-53bf-4333-ae0d-1677f056f6d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce852e2-94fb-4ed7-9a0e-63745896fa05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#List of dependencies\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import difflib\n",
    "## mp4 to .wav converter\n",
    "import ffmpeg\n",
    "\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "SPEECH_REGION = \"xxxx\"\n",
    "\n",
    "SPEECH_KEY=\"xxxx\"\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "############################## GLOBAL VARIABLE SETTING #############################\n",
    "#####################################################################################\n",
    "if not('IS_FIRST_NOTEBOOK_RUN' in locals()):\n",
    "#     os.system(\"apt-get -f -y install xxxxx\")\n",
    "    IS_FIRST_NOTEBOOK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650aa0ed-7c0a-4984-b558-cf71b163c134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container_name=\"xxxx\"\n",
    "storage_name=\"xxxx\"\n",
    "blobkey=\"xxxx\"\n",
    "blobconf = \"fs.azure.account.key.\"+storage_name+\".blob.core.windows.net\"\n",
    "\n",
    "if not any(mount.mountPoint == \"/mnt/\"+storage_name+ \"/\" +container_name for mount in dbutils.fs.mounts()):\n",
    "    print('nothing mounted')\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "          source = \"wasbs://\"+container_name+\"@\"+storage_name+\".blob.core.windows.net\",\n",
    "          mount_point = \"/mnt/\"+storage_name+ \"/\" +container_name,\n",
    "          extra_configs = {blobconf:blobkey})\n",
    "    except Exception as e:\n",
    "        print(\"already mounted. Try to unmount first\")\n",
    "\n",
    "PATH = '/dbfs/mnt/'+storage_name+'/'+container_name +'/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13000b48-e5df-42ad-81d6-3a7d40ed3cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#m4a to wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a37471a-22cf-4072-95f1-7a011f6449b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 4%\r\rReading package lists... 4%\r\rReading package lists... 5%\r\rReading package lists... 5%\r\rReading package lists... 47%\r\rReading package lists... 47%\r\rReading package lists... 47%\r\rReading package lists... 47%\r\rReading package lists... 55%\r\rReading package lists... 58%\r\rReading package lists... 58%\r\rReading package lists... 67%\r\rReading package lists... 67%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 83%\r\rReading package lists... 83%\r\rReading package lists... 91%\r\rReading package lists... 91%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\r\n\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\r\n\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\r\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\r\n0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!apt install -y ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782fd030-70b3-49bf-8f71-2de286d8d128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Detect updated files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab9729d-ab7d-419f-a008-1c1f0ee18980",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Creat Uploaded File Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9701fb48-9801-4983-81ce-547c73f9533c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta, timezone\n",
    "\n",
    "df_file_time = pd.DataFrame(columns = ['FileName','FileTime'])\n",
    "for (dirpath, dirnames, filenames) in os.walk(PATH+\"audio-file-input\"+\"/\"):\n",
    "    for filename in filenames:\n",
    "        if filename.split('.')[1] not in ['csv','txt']:\n",
    "            filepath = dirpath + filename\n",
    "            filetime = os.path.getctime(filepath)\n",
    "            \n",
    "filename_list = []\n",
    "filetime_list = []\n",
    "TAT_list = []\n",
    "now = datetime.now()\n",
    "for (dirpath, dirnames, filenames) in os.walk(PATH+\"audio-file-input\"+\"/\"):\n",
    "    for filename in filenames:\n",
    "        if filename.split('.')[1] not in ['csv','txt']:\n",
    "            filename_list.append(filename.split(\".\")[0])\n",
    "            filepath = dirpath + filename\n",
    "            filetime = os.path.getctime(filepath)\n",
    "            filetime = datetime.fromtimestamp(filetime)\n",
    "            # Create the UTC+7 timezone\n",
    "            utc_plus_7 = timezone(timedelta(hours=7))\n",
    "            filetime = filetime.astimezone(utc_plus_7)\n",
    "            time_gap = now.timetuple().tm_yday - filetime.timetuple().tm_yday\n",
    "            filetime = filetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            filetime_list.append(filetime)\n",
    "            TAT_list.append(time_gap)\n",
    "            \n",
    "df_file_time =  pd.DataFrame({'FileName':filename_list,'FileTime':filetime_list, 'TAT':TAT_list})\n",
    "df_file_time          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d3a0ab-7eb0-44c3-9219-ce38a3377c36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Creat file info (timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d0a846-42a3-4da7-b7d5-d4f8c5e6fe0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import argparse\n",
    "\n",
    "#formats_to_convert = ['.m4a']\n",
    "filename = dbutils.widgets.get('filename')\n",
    "filename = filename.split(\"/\")[1] #define the filename ###.aac. m4a ...\n",
    "if filename.split(\".\")[0] not in ['csv','txt']:\n",
    "\n",
    "    #if filename.endswith(tuple(formats_to_convert)):\n",
    "    filepath = PATH+\"audio-file-input\"+\"/\" + filename\n",
    "    (path, file_extension) = os.path.splitext(filepath)\n",
    "    file_extension_final = file_extension.replace('.', '')\n",
    "    try:\n",
    "        if file_extension_final == 'opus':\n",
    "            track = AudioSegment.from_file(filepath, format=\"ogg\", codec=\"libopus\")\n",
    "        else:\n",
    "            track = AudioSegment.from_file(filepath, file_extension_final)\n",
    "        wav_filename = filename.replace(file_extension_final, 'wav')\n",
    "        wav_path = PATH + \"wav-file-convert\" + '/' + wav_filename\n",
    "        file_handle = track.export(wav_path, format='wav')\n",
    "        print('CONVERTING: ' + str(filepath))\n",
    "        while not os.path.exists(wav_path):\n",
    "            print(wav_path)\n",
    "            file_handle = track.export(wav_path, format='wav')\n",
    "    except:\n",
    "        print(\"ERROR CONVERTING \" + str(filepath))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b1a580-44e9-4587-90e9-a4f247866518",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Speech_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5df25b8-ee8c-4f97-891e-35debd96784e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##speech_language_detection_once_from_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2be8bc0-de51-4b61-906d-4db00643cb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "def recognize_from_file(FilePath):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    #speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "    speech_config = speechsdk.SpeechConfig(endpoint = \"xxxx\", subscription = SPEECH_KEY)\n",
    "    speech_config.speech_recognition_language=\"id-ID\"\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    # <SpeechContinuousRecognitionWithFile>\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=FilePath)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    phrase_list_grammar = speechsdk.PhraseListGrammar.from_recognizer(speech_recognizer)\n",
    "    list_of_mandatory_phrases = [\n",
    "        \"xxxx\"]\n",
    "    \n",
    "    for phrase in list_of_mandatory_phrases:\n",
    "        phrase_list_grammar.addPhrase(phrase)\n",
    "    \n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    all_results = []\n",
    "    def handle_final_result(evt):\n",
    "        all_results.append(evt.result.text)\n",
    "        \n",
    "    speech_recognizer.recognized.connect(handle_final_result)\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    \n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "    #Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "    # </SpeechContinuousRecognitionWithFile>\n",
    "    \n",
    "    all_results = ' '.join(all_results)\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2733aa0-c6d9-465d-894d-40ba31b34045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filename == ###.aac.m4a ...\n",
    "a=''\n",
    "for i in range(4):\n",
    "    try:\n",
    "        if filename.split(\".\")[1] not in ['csv','txt']:\n",
    "            wav_file = filename.split(\".\")[0]+\".wav\"\n",
    "            output = recognize_from_file(PATH+\"wav-file-convert\"+\"/\"+wav_file)\n",
    "            with open(PATH+\"text-file\"+\"/\"+wav_file[:-4]+\".txt\", 'w') as f: #overwrite the file if it existing\n",
    "                if output == 0:\n",
    "                    output= \"\"\n",
    "                f.write(output)\n",
    "    except:\n",
    "        a = 'failed'\n",
    "    if a != 'failed':\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9487ba-75a9-4464-9f87-790e1251d83d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#File Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15cedff6-f020-49dd-8eba-3b9aad944601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_folders = os.listdir(PATH+\"text-file\")\n",
    "speech_list = []\n",
    "speech_name = []\n",
    "\n",
    "for file in text_folders:\n",
    "    split_tup = os.path.splitext(file)\n",
    "    if split_tup[1] == \".txt\":\n",
    "        with open(PATH+\"text-file/\"+file) as f:\n",
    "            speech = f.read()\n",
    "        speech_list.append(speech)\n",
    "        speech_name.append(split_tup[0])\n",
    "\n",
    "df = pd.DataFrame({'FileName':speech_name,\n",
    "                  'Text':speech_list})       \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b594334a-cf74-4c95-a6e4-b5320b3d9c5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Differetiate Individuals & Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c3e9d1-a8fb-4f94-acf0-5cf7eaf4871c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def diff(text):\n",
    "    text = text.replace(\",\",\"\").replace(\".\", \"\").lower()\n",
    "    text = text.replace('resiko','risiko')\n",
    "    num =  [x.replace(\" \",\"\") for x in re.compile(r'\\d[\\d ?]{1,32}\\d', re.M).findall(text[:360])]\n",
    "    for i in num:\n",
    "        text = text.replace(i,i.replace(\" \",\"\"))\n",
    "    \n",
    "    if len(num) == 1:\n",
    "        diff = \"Company\"       \n",
    "    elif len(num) == 2:\n",
    "        diff = 'Individual'\n",
    "    else:\n",
    "        if 'nik' in text:\n",
    "\n",
    "            diff = \"Individual\"\n",
    "        else:\n",
    "            diff = \"Company\"\n",
    "    \n",
    "    return diff\n",
    "                    \n",
    "df['Category'] = df['Text'].apply(diff)\n",
    "\n",
    "df_company = df[df['Category'] == \"Company\"].reset_index(drop=True)\n",
    "df_individual = df[df['Category'] == \"Individual\"].reset_index(drop=True)\n",
    "df_company\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d762b3-0c82-4b4b-b95a-efa79fb5cc07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Template__text_to_find_for_company/individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00b890c-daf9-4f30-9cd6-bd58a1a4eb06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Stemmer & Individual_Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ded267-c590-44a0-80bb-6bd6f7a24c84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "Fact = StemmerFactory()\n",
    "Stemmer = Fact.create_stemmer()\n",
    "\n",
    "\n",
    "text_to_find_for_individual_list =['\"xxxx\"] \n",
    "\n",
    "text_to_find_for_individual_list = [Stemmer.stem(i.replace(\",\",\" \").replace(\".\",\" \").lower()) for i in text_to_find_for_individual_list]\n",
    "\n",
    "from nlp_id.lemmatizer import Lemmatizer \n",
    "lemmatizer = Lemmatizer() \n",
    "text_to_find_for_individual_list = [lemmatizer.lemmatize(i) for i in text_to_find_for_individual_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44fd7680-8662-42cd-be10-19892da32f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Individual_Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63dd370-4e91-46aa-9cc3-2d728b85be9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def indi_test(text):\n",
    "    text = Stemmer.stem(text.replace(\",\",\" \").replace(\".\", \" \").lower())\n",
    "    text = text.replace('resiko','risiko')\n",
    "    num = re.compile(r'\\d[\\d ?]{1,32}\\d', re.M).findall(text)[0].replace(\" \",\"\")\n",
    "    text = text.replace(re.compile(r'\\d[\\d ?]{1,32}\\d', re.M).findall(text)[0],num)\n",
    "\n",
    "    sent_0 = False\n",
    "    sent_1 = False\n",
    "    sent_2 = False\n",
    "    sent_3 = False\n",
    "    sent_4 = False\n",
    "    sent_5 = False  \n",
    "    \n",
    "    \n",
    "    if 'saya' in text.split()[:text.split().index(num)]:\n",
    "        sent_0 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_0 = False\n",
    "    if len(re.findall(r\"xxxx\",text)) > 0: \n",
    "        if str(re.findall(r\"xxxx\",text)[0]) in text:\n",
    "            sent_1 = True\n",
    "            try:\n",
    "                if str(re.findall(r\"xxxx\",text)[0]) == text_to_find_for_individual_list[1]:\n",
    "                    sent_1 = 'Perfect Match'\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        sent_1 = False\n",
    "    if len(re.findall(r\"xxxx\",text))>0:\n",
    "        if str(re.findall(r\"xxxx\"',text)[0]) in text:\n",
    "            sent_2 = True\n",
    "            if str(re.findall(r\"xxxx\",text)[0]) == text_to_find_for_individual_list[2]:\n",
    "                sent_2 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_2 = False\n",
    "    if len(re.findall(r\"xxxx\",text))>0:\n",
    "        if str(re.findall(r\"xxxx\",text)[0]) in text:\n",
    "            sent_3 = True\n",
    "            if str(re.findall(r\"xxxx\",text)[0]) == text_to_find_for_individual_list[3]:\n",
    "                sent_3 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_3 = False\n",
    "    if len(re.findall(r\"xxxx\",text))>0:\n",
    "        if str(re.findall(r\"xxxx\",text)[0]) in text:\n",
    "            sent_4 = True\n",
    "            try:\n",
    "                if str(re.findall(r\"xxxx\",text)[0]) == text_to_find_for_individual_list[4]:\n",
    "                    sent_4 = 'Perfect Match'\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        sent_4 = False\n",
    "    if len(re.findall(r\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r\"xxxx\",text)[0]) in text:\n",
    "            sent_5 = True\n",
    "            try:\n",
    "                if str(re.findall(r\"xxxx\",text)[0]) == text_to_find_for_individual_list[5]:\n",
    "                    sent_5 = 'Perfect Match'\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        sent_5 = False\n",
    "    \n",
    "    \n",
    "    sent = {'sent0':sent_0,\n",
    "            'sent1':sent_1,\n",
    "            'sent2':sent_2,\n",
    "            'sent3':sent_3,\n",
    "            'sent4':sent_4,\n",
    "            'sent5':sent_5}\n",
    "    \n",
    "    Status = \"Refuse\" if False in sent.values() else(\"Accept\" if True in sent.values() else \"Perfect Match\")\n",
    "    sent = {'sent0':sent_0,\n",
    "            'sent1':sent_1,\n",
    "            'sent2':sent_2,\n",
    "            'sent3':sent_3,\n",
    "            'sent4':sent_4,\n",
    "            'sent5':sent_5,\n",
    "            'text_status':Status}\n",
    "    return sent\n",
    "        \n",
    "#pd.DataFrame([indi_test(df.iloc[1]['Text'])]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c8295d-e00f-48e3-aab1-6b352975e749",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Individual File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6a4a9b-87a2-4f27-8dec-d69d23bcde38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1813405632863760>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[0mdf_individual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf_individual\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_temp_indi_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_temp_indi_2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 14\u001B[0;31m \u001B[0mdf_individual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_individual\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_file_time\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'FileName'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'left'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_file_time' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1813405632863760>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mdf_individual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf_individual\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_temp_indi_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_temp_indi_2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mdf_individual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_individual\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_file_time\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'FileName'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'left'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df_file_time' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_file_time' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_temp_indi_1= pd.DataFrame()\n",
    "df_temp_indi_2= pd.DataFrame()\n",
    "for i in df_individual['Text'].apply(indi_Info_extract):\n",
    "    df_temp = pd.DataFrame([i])\n",
    "    df_temp_indi_1 = df_temp_indi_1.append(df_temp,ignore_index=True)\n",
    "\n",
    "    \n",
    "for i in df_individual['Text'].apply(indi_test):\n",
    "    df_temp = pd.DataFrame([i])\n",
    "    df_temp_indi_2 = df_temp_indi_2.append(df_temp,ignore_index=True)\n",
    "\n",
    "\n",
    "df_individual = pd.concat([df_individual,df_temp_indi_1,df_temp_indi_2],axis=1)\n",
    "df_individual = df_individual.merge(df_file_time,on='FileName',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3825f11-9c43-4c44-8c67-20aab8bfa210",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Stemmer & Company_Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b23273-2cad-48eb-aff2-7c4f6d63a95c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_to_find_for_company_list = [\n",
    "\"xxxx\"\n",
    "]\n",
    "text_to_find_for_company_list = [Stemmer.stem(i.replace(\",\",' ').replace(\".\",\" \").lower()) for i in text_to_find_for_company_list]\n",
    "\n",
    "from nlp_id.lemmatizer import Lemmatizer \n",
    "lemmatizer = Lemmatizer() \n",
    "text_to_find_for_company_list = [lemmatizer.lemmatize(i) for i in text_to_find_for_company_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5fe187-d77c-4eec-9eab-4be624b7a8de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Company Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f390d3-9f5f-4347-8d0f-6fd260b62208",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def com_test(text):\n",
    "    text = Stemmer.stem(text.replace(\",\",\" \").replace(\".\", \" \").lower())\n",
    "    text = text.replace('resiko','risiko')\n",
    "\n",
    "    try:\n",
    "        agent_code = re.compile(r'\\d[\\d ?]{1,32}\\d', re.M).findall(text)[0].replace(\" \",\"\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        text = text.replace(re.compile(r'\\d[\\d ?]{1,32}\\d', re.M).findall(text)[0],agent_code)\n",
    "    except:\n",
    "        pass\n",
    "    sent_0 = False\n",
    "    sent_1 = False\n",
    "    sent_2 = False\n",
    "    sent_3 = False\n",
    "    sent_4 = False\n",
    "    sent_5 = False\n",
    "    sent_6 = False \n",
    "\n",
    "    if 'saya' in text.split()[:3]:\n",
    "        sent_0 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_0 = False\n",
    "    \n",
    "    if len(re.findall(r\"xxxx\",text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_1 = True\n",
    "            if str(re.findall(r'\"xxxx\"',text)[0]) == text_to_find_for_company_list[1]:\n",
    "                sent_1 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_1 = False\n",
    "    \n",
    "    if len(re.findall(r'\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_2 = True\n",
    "            if str(re.findall(r'\"xxxx\"',text)[0]) == text_to_find_for_company_list[2]:\n",
    "                sent_2 = 'Perfect Match'\n",
    "    else:\n",
    "        sent2 = False\n",
    "    if len(re.findall(r'\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_3 = True\n",
    "            if str(re.findall(r'\"xxxx\"',text)[0]) == text_to_find_for_company_list[3]:\n",
    "                sent_3 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_3 = False\n",
    "    if len(re.findall(r'\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_4 = True\n",
    "            if str(re.findall(r'\"xxxx\"',text)[0]) == text_to_find_for_company_list[4]:\n",
    "                sent_4 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_4 = False\n",
    "    if len(re.findall(r'\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_5 = True\n",
    "            if str(re.findall(r\"xxxx\"i',text)[0]) == text_to_find_for_company_list[5]:\n",
    "                sent_5 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_5 = False\n",
    "    if len(re.findall(r'\"xxxx\"',text))>0:\n",
    "        if str(re.findall(r'\"xxxx\"',text)[0]) in text:\n",
    "            sent_6 = True\n",
    "            if str(re.findall(r'\"xxxx\"',text)[0]) == text_to_find_for_company_list[6]:\n",
    "                sent_6 = 'Perfect Match'\n",
    "    else:\n",
    "        sent_6 = False\n",
    "        \n",
    "    sent = {'sent0':sent_0,\n",
    "            'sent1':sent_1,\n",
    "            'sent2':sent_2,\n",
    "            'sent3':sent_3,\n",
    "            'sent4':sent_4,\n",
    "            'sent5':sent_5,\n",
    "            'sent6':sent_6}\n",
    "    Status = \"Refuse\" if False in sent.values() else(\"Accept\" if True in sent.values() else \"Perfect Match\")\n",
    "    sent = {'sent0':sent_0,\n",
    "            'sent1':sent_1,\n",
    "            'sent2':sent_2,\n",
    "            'sent3':sent_3,\n",
    "            'sent4':sent_4,\n",
    "            'sent5':sent_5,\n",
    "            'sent6':sent_6,\n",
    "            'text_status':Status}\n",
    "    \n",
    "    return sent\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5a2521-7370-45a0-be44-79848d4c313c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_temp_comp_1= pd.DataFrame()\n",
    "df_temp_comp_2= pd.DataFrame()\n",
    "for i in df_company['Text'].apply(comp_Info_extract):\n",
    "    df_temp = pd.DataFrame([i])\n",
    "    df_temp_comp_1 = df_temp_comp_1.append(df_temp,ignore_index=True)\n",
    "    \n",
    "for i in df_company['Text'].apply(com_test):\n",
    "    df_temp = pd.DataFrame([i])\n",
    "    df_temp_comp_2 = df_temp_comp_2.append(df_temp,ignore_index=True)\n",
    "\n",
    "\n",
    "df_company = pd.concat([df_company,df_temp_comp_1,df_temp_comp_2],axis=1)\n",
    "df_company = df_company.merge(df_file_time,on='FileName',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e5f1ba-4c66-4258-921e-ebd7a14ef1bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Match with .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c473997-2a70-4fea-9e07-202858ec1f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = PATH+\"audio-file-input\"\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        #if filename != \"file_info.csv\":\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        a = pd.read_csv(filepath)\n",
    "        a['FileName'] = filename.split(\".\")[0]\n",
    "        df_list.append(a)\n",
    "df_csv = pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e10a73-cf21-4306-bcb4-f4eeb05f2b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_company = df_company.merge(df_csv,on=\"FileName\",how='left')\n",
    "df_individual = df_individual.merge(df_csv,on=\"FileName\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4382347-ac96-45d9-ace4-ee31d24b3b8e",
     "showTitle": true,
     "title": "DataBase"
    }
   },
   "source": [
    "#Data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b08144-4bfa-4ff7-bb86-92be56a5c597",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
    "curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list \n",
    "apt-get update\n",
    "ACCEPT_EULA=Y apt-get install msodbcsql17\n",
    "apt-get -y install unixodbc-dev\n",
    "sudo apt-get install python3-pip -y\n",
    "pip3 install --upgrade pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05df5739-16c4-4511-9dd4-92cef7a75687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"/databricks/init/<YourClusterName>/pyodbc-install.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "sudo apt-get update\n",
    "sudo apt-get -q -y install unixodbc unixodbc-dev\n",
    "sudo apt-get -q -y install python3-dev\n",
    "/databricks/python/bin/pip install pyodbc\n",
    "\"\"\", True)\n",
    "\n",
    "import urllib\n",
    "\n",
    "import pyodbc\n",
    "for driver in pyodbc.drivers():\n",
    "    print(driver)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = \"xxxx\"\n",
    "database = \"xxxx\"\n",
    "username = \"xxxx\"\n",
    "password = \"xxxx\"\n",
    "\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "odbc_str = 'DRIVER='+driver+';SERVER='+server+';PORT=1433;UID='+username+';DATABASE='+ database + ';PWD='+ password\n",
    "connect_str = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(odbc_str)\n",
    "engine = create_engine(connect_str)\n",
    "\n",
    "#used to create a table, will overwrite \n",
    "\n",
    "def to_sql(df, table):\n",
    "    df.to_sql(table, engine, if_exists = \"replace\", index=False, chunksize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "693c551a-0075-453c-b986-ffa4234946fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conn = pyodbc.connect(odbc_str)\n",
    "sql = 'SELECT * FROM [dbo].[\"xxxx\"]'\n",
    "df = pd.read_sql(sql, conn)\n",
    "duplicated_values = df.duplicated('FileName', keep=False)\n",
    "\n",
    "# Drop the rows with duplicate values in 'col1' while keeping the latest occurrence\n",
    "df = df[~duplicated_values]\n",
    "\n",
    "\n",
    "df_result = pd.merge(df_result,df[['FileName','Manual Review']],on='FileName',how='left')\n",
    "df_result.rename(columns = {'Manual Review_y':'Manual Review'}, inplace=True)\n",
    "df_result.drop('Manual Review_x', inplace =True, axis=1)\n",
    "\n",
    "filename = dbutils.widgets.get('filename')\n",
    "filename = filename.split(\"/\")[1]\n",
    "# try:\n",
    "#     df_result.loc[df_result['FileName'] == filename.split(\".\")[0], 'Manual Review'] = ''\n",
    "# except:\n",
    "#     pass\n",
    "try:\n",
    "    df_result.loc[df_result['FileName'].astype(int) == int(filename.split(\".\")[0]), 'Manual Review'] = ''\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_result = df_result.drop_duplicates(subset='FileName', keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d6c704-1962-4d57-b3fa-39ae5c1cd1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    to_sql(df_result,\"\"xxxx\"\")\n",
    "except: \n",
    "    to_sql(df_result,\"\"xxxx\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b7a2962-f3dc-4c6c-8b5c-0853d3e898bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_attempts = 2\n",
    "success = False\n",
    "\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        conn = pyodbc.connect(odbc_str)\n",
    "        cur = conn.cursor()\n",
    "        sql = '''ALTER TABLE \"xxxx\" ADD id INT NOT NULL PRIMARY KEY IDENTITY'''\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "        break\n",
    "    except pyodbc.Error as e:\n",
    "        if e.sqlstate == '23000':\n",
    "            print(f\"Error: Cannot create a non-unique primary key for the 'id' column. Retrying... (Attempt {attempt + 1})\")\n",
    "        else:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "if not success:\n",
    "    print(\"Failed to add 'id' as primary key after multiple attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6198757-a41c-4af0-a3df-33d86beb7984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1813405632863783,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "speech_to_text_ID.export",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
