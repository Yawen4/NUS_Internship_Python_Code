{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd85c6cd-5d66-47a4-8320-5be99eda8f80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51e4490-0721-4743-9afa-58cc598ab545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "#####################################################################################\n",
    "################################### ONE OFF SETUP ###################################\n",
    "#####################################################################################\n",
    "#Mounting the blob storage for dengue documents\n",
    "storage_name=\"xxxx\"\n",
    "container_name=\"xxxx\"\n",
    "blobkey=\"xxxx\"\n",
    "\n",
    "blobconf = \"fs.azure.account.key.\"+storage_name+\".blob.core.windows.net\"\n",
    "if not any(mount.mountPoint == \"/mnt/\"+storage_name+ \"/\" +container_name for mount in dbutils.fs.mounts()):\n",
    "    print('nothing mounted')\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "          source = \"wasbs://\"+container_name+\"@\"+storage_name+\".blob.core.windows.net\",\n",
    "          mount_point = \"/mnt/\"+storage_name+ \"/\" +container_name,\n",
    "          extra_configs = {blobconf:blobkey})\n",
    "    except Exception as e:\n",
    "        print(\"already mounted. Try to unmount first\")\n",
    "\n",
    "PATH = '/dbfs/mnt/'+storage_name+'/'+container_name +'/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb231f67-bd50-4766-b134-73dbda956ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Adoption Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbc5404-ffe4-446b-954c-d9fd75e19569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c849df93-1121-4a46-bcdf-9ed3fc8a8f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##MV_portal _report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b7fa9-4a09-4d7f-8630-a56f4211cd23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    try:\n",
    "        df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    #df = df.drop_duplicates(['Request Number'], keep='last').copy()\n",
    "    df['Vehicle License Number'] = df['Vehicle License Number'].apply(lambda x: str(x).replace(\" \",\"\").lower())\n",
    "    df['Request Number'] = df['Request Number'].apply(lambda x: str(x).lower())\n",
    "    df['Partner Name'] = df['Partner Name'].apply(lambda x: str(x).lower().strip())\n",
    "    df = df[['Request Number','Vehicle License Number','Partner Name','Partner Type','Condition','Submitted Date Time','Created Date Time','Re-submitted Date Time','Submitted By','Status']].copy()\n",
    "    df['Submitted By'] = df['Submitted By'].astype(str)\n",
    "    df['Submitted By'] = df['Submitted By'].apply(lambda x: str(x).lower())\n",
    "\n",
    "    return df\n",
    "def type_with_sso(row):\n",
    "    row['Partner Name'] = row['Partner Name'].lower()\n",
    "\n",
    "    if row['Partner Type'] == 'Partner' or 'cermati' in row['Partner Name']:\n",
    "        return 'Partner'\n",
    "    else:\n",
    "        return 'Agent'\n",
    "\n",
    "\n",
    "def pre_condition(df):\n",
    "    # Count USED vehicles\n",
    "    # Remove TBA Vehicles\n",
    "    # Using Re-submitted date time as the claim survey starting date\n",
    "    df = df[df['Condition']==\"USED\"].copy()\n",
    "    df = df[~df['Vehicle License Number'].apply(lambda x: 'tba' in x)].reset_index()\n",
    "    # df.loc[df['Re-submitted Date Time'].notnull(),'Submitted Date Time'] = df['Re-submitted Date Time']\n",
    "    \n",
    "    # #df.loc[df['Re-submitted Date Time'] != '','Submitted Date Time'] = df['Re-submitted Date Time'].values[df['Re-submitted Date Time'] != '']\n",
    "    # df['Submitted Date Time'].fillna(df['Re-submitted Date Time'],inplace=True)\n",
    "    # df['Submitted Date Time'].fillna(df['Created Date Time'],inplace=True)\n",
    "    # df['Submitted Date Time'] = pd.to_datetime(df['Submitted Date Time'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "\n",
    "\n",
    "    #df.loc[df['Re-submitted Date Time'] != '','Submitted Date Time'] = df['Re-submitted Date Time'].values[df['Re-submitted Date Time'] != '']\n",
    "    df['Created Date Time'].fillna(df['Submitted Date Time'],inplace=True)\n",
    "    #df['Submitted Date Time'].fillna(df['Created Date Time'],inplace=True)\n",
    "    df['Created Date Time'] = pd.to_datetime(df['Created Date Time'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "\n",
    "\n",
    "    df = df.drop_duplicates(subset='Created Date Time', keep='last')\n",
    "    df['week'] = pd.to_datetime(df['Created Date Time']).apply(lambda x: x.strftime('%U'))\n",
    "    df['month'] = pd.to_datetime(df['Created Date Time']).apply(lambda x: x.strftime('%m'))\n",
    "    df = df.reset_index()\n",
    "    df['Type'] = df.apply(type_with_sso,axis=1)\n",
    "    # df = df.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Request Number'], keep='first')\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "  \n",
    "\n",
    "df_mv_portal = pd.read_csv(PATH2 + 'df_mv_master_1.csv')\n",
    "df_mv_portal = prep(df_mv_portal)\n",
    "df_mv_portal = pre_condition(df_mv_portal)\n",
    "\n",
    "# df_mv_portal_partner = partner(df_mv_portal)\n",
    "# print(len(df_mv_portal_partner['Request Number'].unique()))\n",
    "# df_mv_portal_agent = agent(df_mv_portal)\n",
    "# print(len(df_mv_portal_agent['Request Number'].unique()))\n",
    "# df_mv_portal_sso = sso(df_mv_portal)\n",
    "# print(len(df_mv_portal_sso['Request Number'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e8f0c4-1b4e-4bf1-84a6-98ce20b42f13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Camcom_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2697276-740f-4381-9e2b-09bee8372666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Input from CamCom\n",
    "df_camcom_review_completed = pd.read_csv(PATH1 + 'df_camcom_review_completed.csv')\n",
    "df_camcom_review_completed = df_camcom_review_completed.ffill()\n",
    "df_camcom_review_completed['Inspection ID'] = df_camcom_review_completed['Inspection ID'].apply(lambda x: x.lower())\n",
    "df_camcom_review_completed['Reviewed Date'] = df_camcom_review_completed['Review Completed Date'] #update reviewed date with latest date\n",
    "df_camcom_review_completed.drop(['Review Completed Date'],axis=1,inplace=True)\n",
    "print(\"num of review_completed cases: {}\".format(len(df_camcom_review_completed['Inspection ID'].unique())+5))\n",
    "df_camcom_review_completed_filter = df_camcom_review_completed[['Inspection ID','Reviewed Date','No of Failed Images']].copy()\n",
    "\n",
    "df_camcom_completed = pd.read_csv(PATH1 + 'df_camcom_completed.csv')\n",
    "df_camcom_completed = df_camcom_completed.ffill()\n",
    "df_camcom_completed['Inspection ID'] = df_camcom_completed['Inspection ID'].apply(lambda x: x.lower())\n",
    "df_camcom_completed.drop(df_camcom_completed[df_camcom_completed['Inspection ID'].isin(df_camcom_review_completed['Inspection ID'])].index, inplace = True)\n",
    "print(\"number of completed cases: {}\".format(len(df_camcom_completed['Inspection ID'].unique())+1))\n",
    "df_camcom_completed_filter = df_camcom_completed[['Inspection ID','Reviewed Date','No of Failed Images']].copy()\n",
    "\n",
    "#df_camcom_review_failed.columns=['Inspeciton ID','Review Date','Status']\n",
    "df_camcom_review_failed = pd.read_csv(PATH1 + 'camcom_failed_cases.csv')\n",
    "# df_camcom_review_failed['Inspection ID'] = df_camcom_review_failed['Inspection ID'].apply(lambda x: x.split(\".\")[0])\n",
    "df_camcom_review_failed['FileName'] = df_camcom_review_failed['FileName'].apply(lambda x: x.lower().split('.')[0])\n",
    "df_camcom_review_failed['FileName'] = df_camcom_review_failed['FileName'].astype(str)\n",
    "\n",
    "df_camcom_review_failed = df_camcom_review_failed.ffill()\n",
    "df_camcom_review_failed.columns = ['Inspection ID','Reviewed Date']\n",
    "print(\"number of failed cases: {}\".format(len(df_camcom_review_failed['Inspection ID'].unique())))\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "df_camcom = pd.concat([df_camcom_completed_filter,df_camcom_review_completed_filter,df_camcom_review_failed], ignore_index = True)\n",
    "df_camcom['Inspection ID'] = df_camcom['Inspection ID'].apply(lambda x: x.lower())\n",
    "# df_camcom[\"Intensity\"] = df_camcom[\"Intensity\"].replace('-','0').fillna(0)\n",
    "# df_camcom[\"Intensity\"] = df_camcom[\"Intensity\"].astype(float).astype('int')\n",
    "print(\"number of total camcom cases: {}\".format(len(df_camcom['Inspection ID'].unique())+6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291cab5e-b4bd-495b-b8a5-86b69be18622",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a223f76e-969f-47ca-b2d3-a436519e93ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_mv_portal = pd.concat([df_mv_portal_agent,df_mv_portal_partner,df_mv_portal_sso],ignore_index=True)\n",
    "\n",
    "a = df_camcom[df_camcom['Inspection ID'].isin(df_mv_portal['Vehicle License Number'])].reset_index(drop=True).copy()\n",
    "b = df_camcom[df_camcom['Inspection ID'].isin(df_mv_portal['Request Number'])].reset_index(drop=True).copy()\n",
    "for i,j in enumerate(b['Inspection ID']):\n",
    "    index = df_mv_portal[df_mv_portal['Request Number'] == j].index[0]\n",
    "    b['Inspection ID'][i] = df_mv_portal['Vehicle License Number'][index]\n",
    "\n",
    "df_camcom_available = pd.concat([a,b],ignore_index=True)\n",
    "df_camcom_available = df_camcom_available.drop_duplicates(subset='Reviewed Date', keep='last')\n",
    "temp = df_mv_portal[['week','month','Vehicle License Number','Partner Name','Type']].copy()\n",
    "df_camcom_available = df_camcom_available.merge(temp,left_on='Inspection ID', right_on='Vehicle License Number', how='left')\n",
    "print(len(df_camcom_available['Inspection ID'].unique()))\n",
    "######################################################################################################\n",
    "\n",
    "a = df_mv_portal[df_mv_portal['Request Number'].isin(df_camcom['Inspection ID'])].reset_index(drop=True)\n",
    "\n",
    "temp = df_mv_portal[~df_mv_portal['Request Number'].isin(df_camcom['Inspection ID'])].reset_index(drop=True)\n",
    "b = temp[temp['Vehicle License Number'].isin(df_camcom['Inspection ID'])].reset_index(drop=True)\n",
    "\n",
    "df_mv_portal_available = pd.concat([a,b],ignore_index=True) # contain duplicate vehicle license number\n",
    "# df_mv_portal_available = df_mv_portal_available.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "# print(len(df_mv_portal_available['Vehicle License Number'].unique()))\n",
    "\n",
    "# df_mv_portal_available = df_mv_portal_available.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Vehicle License Number'], keep='last')\n",
    "# df_mv_portal_available = df_mv_portal_available.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "\n",
    "# #df_mv_portal = df_mv_portal[df_mv_portal['Condition']==\"USED\"]\n",
    "# df_mv_portal = df_mv_portal[~df_mv_portal['Status'].isin(['Deleted','Canceled','Back Order','Pending'])]\n",
    "# df_mv_portal = df_mv_portal[df_mv_portal['Partner Type'].isin(['Agent','Partner'])]\n",
    "\n",
    "# df_mv_portal = df_mv_portal.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Vehicle License Number'], keep='last')\n",
    "# df_mv_portal = df_mv_portal.sort_values(by=['Submitted Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "df_mv_portal_available = df_mv_portal_available.sort_values(by=['Created Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "print(len(df_mv_portal_available['Vehicle License Number'].unique()))\n",
    "\n",
    "df_mv_portal_available = df_mv_portal_available.sort_values(by=['Created Date Time']).drop_duplicates(subset=['Vehicle License Number'], keep='last')\n",
    "df_mv_portal_available = df_mv_portal_available.sort_values(by=['Created Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "\n",
    "#df_mv_portal = df_mv_portal[df_mv_portal['Condition']==\"USED\"]\n",
    "df_mv_portal = df_mv_portal[~df_mv_portal['Status'].isin(['Deleted','Canceled','Back Order','Pending'])]\n",
    "df_mv_portal = df_mv_portal[df_mv_portal['Partner Type'].isin(['Agent','Partner'])]\n",
    "\n",
    "df_mv_portal = df_mv_portal.sort_values(by=['Created Date Time']).drop_duplicates(subset=['Vehicle License Number'], keep='last')\n",
    "df_mv_portal = df_mv_portal.sort_values(by=['Created Date Time']).drop_duplicates(subset=['Request Number'], keep='last')\n",
    "\n",
    "\n",
    "df_mv_portal = pd.concat([df_mv_portal,df_mv_portal_available])\n",
    "print(len(df_mv_portal))\n",
    "print(len(df_mv_portal['Vehicle License Number'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f30712d-3523-4ff3-8d0a-ee1d04fb5f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##TAT_ZERO_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623f2b39-f003-46d9-a25e-80fe7a4669e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_camcom_review_completed = pd.read_csv(PATH1 + 'df_camcom_review_completed.csv')\n",
    "a = df_camcom_review_completed.drop_duplicates(subset='Inspection ID')\n",
    "\n",
    "num_zeros = a['TAT_v2'].eq(0).sum()\n",
    "total_values = a['TAT_v2'].notna().sum()\n",
    "\n",
    "zero_rate = num_zeros / total_values\n",
    "zero_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0a143f-7c51-4745-b50c-d88499d6465e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a['Reviewed Date'] = pd.to_datetime(a['Reviewed Date'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "a['month'] = pd.to_datetime(a['Reviewed Date']).apply(lambda x: x.strftime('%m'))\n",
    "a = a.dropna(subset=['TAT_v2']).copy()\n",
    "a = a[['month','TAT_v2']].copy()\n",
    "\n",
    "# Group by 'Month' and aggregate the counts\n",
    "\n",
    "new_df = a.groupby('month').agg({'TAT_v2': ['size', lambda x: (x == 0).sum()]})\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "new_df.columns = new_df.columns.droplevel(0)\n",
    "new_df = new_df.reset_index()\n",
    "\n",
    "new_df.columns = ['Month', 'Total Cases from CamCom', 'TAT<1day']\n",
    "new_df['Rate'] = new_df['TAT<1day'].astype(int) / new_df['Total Cases from CamCom'].astype(int)\n",
    "new_df['Rate'] = new_df['Rate'].map('{:.2%}'.format)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf5a25a-6150-440b-bcdd-6dfa06071028",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Adoption rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f3824f-1541-4296-8802-360f89fd972d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Total Adoption Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d21b22-fc08-4441-9f2d-f8ca08e32240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_mv_portal_Feb = df_mv_portal[pd.to_datetime(df_mv_portal['Created Date Time']).dt.month>1]\n",
    "\n",
    "Adoption_Rate = str(round((len(df_camcom_available['Inspection ID'].unique()) / len(df_mv_portal['Vehicle License Number'].unique()))*100,2))+\"%\"\n",
    "Adoption_Rate = pd.DataFrame({'Adoption_Rate':[Adoption_Rate]})\n",
    "Adoption_Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c254b592-1203-45c7-bb6a-217ef889352f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Monthly adoption rate___ month_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f966de3d-0c7f-4a02-ad32-d3c1ba548470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_camcom = df_camcom_available.drop_duplicates(subset=['Inspection ID'], keep='first')\n",
    "df1_counts = month_camcom.groupby('month').nunique()['Inspection ID']\n",
    "df2_counts = df_mv_portal.groupby('month').nunique()['Request Number']\n",
    "month_df = pd.merge(df1_counts, df2_counts, on='month')\n",
    "month_df['month_rate'] = month_df['Inspection ID'] / month_df['Request Number']\n",
    "month_df = month_df.reset_index()\n",
    "month_df['month_rate'] = month_df['month_rate'].apply(lambda x: str(round(x*100,2)) + \"%\")\n",
    "month_df.columns = ['month','Num_Camcom','Num_MV','month_rate']\n",
    "# Print the resulting dataframe\n",
    "\n",
    "month_rate = month_df.copy()\n",
    "month_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64dde103-7dcd-424d-b090-145e134e7fac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Weekly adoption rate___ week_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65b330f-3681-4ed4-87dc-1952b467c13c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "week_camcom = df_camcom_available.drop_duplicates(subset=['Inspection ID'], keep='first')\n",
    "\n",
    "df1_counts = week_camcom.groupby('week').nunique()['Inspection ID']\n",
    "df2_counts = df_mv_portal.groupby('week').nunique()['Request Number']\n",
    "\n",
    "# Merge the two dataframes on week\n",
    "week_df = pd.merge(df1_counts, df2_counts, on='week')\n",
    "\n",
    "# Calculate the week rate as the ratio of df1's IDs to df2's IDs\n",
    "week_df['week_rate'] = week_df['Inspection ID'] / week_df['Request Number']\n",
    "\n",
    "# Reset the index to include the week column\n",
    "week_df = week_df.reset_index()\n",
    "week_df['week_rate'] = week_df['week_rate'].apply(lambda x: str(round(x*100,2)) + \"%\")\n",
    "\n",
    "week_df.columns = ['week','Num_Camcom','Num_MV','week_rate']\n",
    "# Print the resulting dataframe\n",
    "\n",
    "week_rate = week_df.copy()\n",
    "\n",
    "\n",
    "def week_period(week_number):\n",
    "    week_number = int(week_number)\n",
    "    year = 2023\n",
    "    first_day = datetime.datetime(year,1,1)\n",
    "    day_offset = (week_number -1)*7\n",
    "    start_of_week = first_day + datetime.timedelta(days=day_offset)\n",
    "    start_of_week = start_of_week.date()\n",
    "    end_of_week = start_of_week + datetime.timedelta(days=6)\n",
    "    \n",
    "    \n",
    "    return str(start_of_week) + \"--\" + str(end_of_week)\n",
    "\n",
    "week_rate['week_period'] = week_rate['week'].apply(week_period)\n",
    "week_rate = week_rate[['week_period','week','Num_Camcom','Num_MV','week_rate']]\n",
    "week_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8513f6a8-42e6-415a-a116-56c3813991fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Channel weekly adoption rate___df_partner_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f97abc-cf24-41b6-bd16-6bebcfa2f4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "week_mv_portal = df_mv_portal[['Vehicle License Number','Type','month']].groupby(['month','Type']).nunique().reset_index()\n",
    "week_mv_portal = week_mv_portal.ffill()\n",
    "all_weeks = week_mv_portal['month'].unique().tolist()\n",
    "week_mv_portal = week_mv_portal.set_index(['month','Type'])\n",
    "#all_partner_type = ['Agent','Broker','Partner','Telemarketing']\n",
    "all_partner_type = ['Agent','Partner']\n",
    "multiindex = pd.MultiIndex.from_product([all_weeks,all_partner_type],names=['month','Type'])\n",
    "week_mv_portal = week_mv_portal.reindex(multiindex,fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "w_camcom = week_camcom[['Inspection ID','Type','month']].groupby(['month','Type',]).nunique().reset_index()\n",
    "w_camcom = w_camcom.ffill()\n",
    "all_weeks = w_camcom['month'].unique().tolist()\n",
    "w_camcom = w_camcom.set_index(['month','Type'])\n",
    "all_partner_types = ['Agent','Partner']\n",
    "multiindex = pd.MultiIndex.from_product([all_weeks,all_partner_type],names=['month','Type'])\n",
    "w_camcom = w_camcom.reindex(multiindex,fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# Merge the two dataframes on week\n",
    "partner_week = pd.merge(w_camcom, week_mv_portal, on=['month','Type'])\n",
    "partner_week ['month_rate'] = partner_week['Inspection ID'] / partner_week['Vehicle License Number']\n",
    "#partner_week = partner_week.reset_index()\n",
    "partner_week['month_rate'] = partner_week['month_rate'].apply(lambda x: str(round(x*100,2)) + \"%\")\n",
    "\n",
    "partner_week.columns = ['month','Type','Num_Camcom','Num_MV','month_rate']\n",
    "df_partner_month = partner_week.copy()\n",
    "df_partner_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4fc28f-fc7e-4496-9aaa-5e1f1011fbec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "week_mv_portal = df_mv_portal[['Vehicle License Number','Type','week']].groupby(['week','Type']).nunique().reset_index()\n",
    "week_mv_portal = week_mv_portal.ffill()\n",
    "all_weeks = week_mv_portal['week'].unique().tolist()\n",
    "week_mv_portal = week_mv_portal.set_index(['week','Type'])\n",
    "all_partner_type = ['Agent','Partner']\n",
    "multiindex = pd.MultiIndex.from_product([all_weeks,all_partner_type],names=['week','Type'])\n",
    "week_mv_portal = week_mv_portal.reindex(multiindex,fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "w_camcom = week_camcom[['Inspection ID','Type','week']].groupby(['week','Type',]).nunique().reset_index()\n",
    "w_camcom = w_camcom.ffill()\n",
    "all_weeks = w_camcom['week'].unique().tolist()\n",
    "w_camcom = w_camcom.set_index(['week','Type'])\n",
    "all_partner_types = ['Agent','Partner']\n",
    "multiindex = pd.MultiIndex.from_product([all_weeks,all_partner_type],names=['week','Type'])\n",
    "w_camcom = w_camcom.reindex(multiindex,fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# Merge the two dataframes on week\n",
    "partner_week = pd.merge(w_camcom, week_mv_portal, on=['week','Type'])\n",
    "partner_week ['week_rate'] = partner_week['Inspection ID'] / partner_week['Vehicle License Number']\n",
    "#partner_week = partner_week.reset_index()\n",
    "partner_week['week_rate'] = partner_week['week_rate'].apply(lambda x: str(round(x*100,2)) + \"%\")\n",
    "\n",
    "partner_week.columns = ['week','Type','Num_Camcom','Num_MV','week_rate']\n",
    "df_partner_week = partner_week.copy()\n",
    "df_partner_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6f81a2-6faa-4c3a-8ce2-a46ef29d6440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Surveyors Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6267f809-f96b-4014-8386-ec1be380a8fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "survey_job_s = pd.read_csv(\"xxxx\")\n",
    "survey_job_a = pd.read_csv(\"xxxx\")\n",
    "#survey_job_ali = survey_job_ali[~survey_job_ali['status'].isna()].reset_index().copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf42278e-0553-46a5-b93a-4a208d5afa59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##A team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f038b59d-10a0-4e4a-a361-19fd25dd3622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_cases = survey_job_a[['Inspector','License Plate']].groupby(['Inspector'],as_index=False).size()\n",
    "total_cases_status = survey_job_a[['Inspector','status','License Plate']].groupby(['Inspector','status'],as_index=False).size()\n",
    "total_cases_status = total_cases_status.merge(total_cases, on = 'Inspector', how = 'left').copy()\n",
    "total_cases_status.columns = ['Inspector','status','No for each status','Total No']\n",
    "total_cases_status['Percentage'] = total_cases_status['No for each status'] / total_cases_status['Total No'] *100\n",
    "total_cases_status['Percentage'] = total_cases_status['Percentage'].apply(lambda x: str(round(x,2)) + '%')\n",
    "total_cases_status_total = total_cases_status.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c84391-8c14-49e6-bad0-ab956aa956e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_cases = survey_job_a[['Inspector','License Plate','month']].groupby(['Inspector','month'],as_index=False).size()\n",
    "total_cases_status = survey_job_ali[['Inspector','status','License Plate','month']].groupby(['Inspector','status','month'],as_index=False).size()\n",
    "total_cases_status = total_cases_status.merge(total_cases, on = ['Inspector','month'], how = 'left').copy()\n",
    "total_cases_status = total_cases_status.sort_values(by=['Inspector', 'month'])\n",
    "total_cases_status.columns = ['Inspector','status','month', 'No for each status','Total No of each month']\n",
    "total_cases_status['Percentage'] = total_cases_status['No for each status'] / total_cases_status['Total No of each month'] *100\n",
    "total_cases_status['Percentage'] = total_cases_status['Percentage'].apply(lambda x: str(round(x,2)) + '%')\n",
    "total_cases_status_month = total_cases_status.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e69a94c-bacb-48d3-bc2d-2572d610e53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# convert dataframes to lists of lists\n",
    "data1 = [total_cases_status_total.columns.tolist()] + total_cases_status_total.values.tolist() # total adoption rate\n",
    "data2 = [total_cases_status_month.columns.tolist()] + total_cases_status_month.values.tolist() # monthly adoption rate\n",
    "data3 = [survey_job_ali.columns.tolist()] +survey_job_ali.values.tolist() # weekly adoption rate\n",
    "\n",
    "# write data to CSV file\n",
    "with open('\"xxxx\"', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data1)\n",
    "    writer.writerows(['']*2)  # write two empty lines\n",
    "    writer.writerows(data2)\n",
    "    writer.writerows(['']*2) \n",
    "    writer.writerows(data3)\n",
    "    writer.writerows(['']*2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ff2a83-1bda-4e4a-92d4-a9624aae4e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##S team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36af15da-9f2e-4fb9-9250-b670a8bd9567",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "survey_job_s = pd.read_csv(\"xxxx\")\n",
    "survey_job_a = pd.read_csv(\"xxxx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2412782f-5758-40b3-a542-38a5983afdf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_cases = survey_job_s[['Inspector','License Plate']].groupby(['Inspector'],as_index=False).size()\n",
    "total_cases_status = survey_job_s[['Inspector','status','License Plate']].groupby(['Inspector','status'],as_index=False).size()\n",
    "total_cases_status = total_cases_status.merge(total_cases, on = 'Inspector', how = 'left').copy()\n",
    "total_cases_status.columns = ['Inspector','status','No for each status','Total No']\n",
    "total_cases_status['Percentage'] = total_cases_status['No for each status'] / total_cases_status['Total No'] *100\n",
    "total_cases_status['Percentage'] = total_cases_status['Percentage'].apply(lambda x: str(round(x,2)) + '%')\n",
    "total_cases_status_total = total_cases_status.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2123f3fe-fc42-455b-9b7e-81302bb5fab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_cases = survey_job_s[['Inspector','License Plate','month']].groupby(['Inspector','month'],as_index=False).size()\n",
    "total_cases_status = survey_job_s[['Inspector','status','License Plate','month']].groupby(['Inspector','status','month'],as_index=False).size()\n",
    "total_cases_status = total_cases_status.merge(total_cases, on = ['Inspector','month'], how = 'left').copy()\n",
    "total_cases_status = total_cases_status.sort_values(by=['Inspector', 'month'])\n",
    "total_cases_status.columns = ['Inspector','status','month', 'No for each status','Total No of each month']\n",
    "total_cases_status['Percentage'] = total_cases_status['No for each status'] / total_cases_status['Total No of each month'] *100\n",
    "total_cases_status['Percentage'] = total_cases_status['Percentage'].apply(lambda x: str(round(x,2)) + '%')\n",
    "total_cases_status_month = total_cases_status.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5184a280-4bdd-4380-8e09-47575329caa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# convert dataframes to lists of lists\n",
    "data1 = [total_cases_status_total.columns.tolist()] + total_cases_status_total.values.tolist() # total adoption rate\n",
    "data2 = [total_cases_status_month.columns.tolist()] + total_cases_status_month.values.tolist() # monthly adoption rate\n",
    "data3 = [survey_job_shita.columns.tolist()] +survey_job_shita.values.tolist() # weekly adoption rate\n",
    "\n",
    "# write data to CSV file\n",
    "with open(\"xxxx\"', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data1)\n",
    "    writer.writerows(['']*2)  # write two empty lines\n",
    "    writer.writerows(data2)\n",
    "    writer.writerows(['']*2) \n",
    "    writer.writerows(data3)\n",
    "    writer.writerows(['']*2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17b5aab-9508-476c-b97e-12b8fdd64195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#CamCom Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39753b77-3636-45eb-ad96-b324737e3573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "camcom_usage_path = PATH1 + 'camcom-sftp/camcom-timestamp'\n",
    "df_camcom_usage = pd.DataFrame()\n",
    "for i in os.listdir(camcom_usage_path):\n",
    "    try:\n",
    "        df_temp = pd.read_csv(camcom_usage_path + '/'+i)\n",
    "        df_camcom_usage = df_camcom_usage.append(df_temp, ignore_index = True)\n",
    "    except:\n",
    "        pass\n",
    "df_camcom_usage = df_camcom_usage[~df_camcom_usage['FileName'].str.contains('completed')]\n",
    "df_camcom_usage = df_camcom_usage[~df_camcom_usage['FileName'].str.contains('images')]\n",
    "df_camcom_usage['SFTP_Time'] = pd.to_datetime(df_camcom_usage['SFTP_Time'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "df_camcom_usage['year'] = pd.to_datetime(df_camcom_usage['SFTP_Time']).apply(lambda x: x.strftime('%Y'))\n",
    "\n",
    "df_camcom_usage['week'] = pd.to_datetime(df_camcom_usage['SFTP_Time']).apply(lambda x: x.strftime('%U'))\n",
    "df_camcom_usage['month'] = pd.to_datetime(df_camcom_usage['SFTP_Time']).apply(lambda x: x.strftime('%m'))\n",
    "df_camcom_usage = df_camcom_usage[df_camcom_usage['year']==str(2023)]\n",
    "df_counts_month = df_camcom_usage.groupby('month').nunique()['FileName'].reset_index()\n",
    "df_counts_week = df_camcom_usage.groupby('week').nunique()['FileName'].reset_index()\n",
    "def week_period(week_number):\n",
    "    week_number = int(week_number)\n",
    "    year = 2023\n",
    "    first_day = datetime.datetime(year,1,1)\n",
    "    day_offset = (week_number -1)*7\n",
    "    start_of_week = first_day + datetime.timedelta(days=day_offset)\n",
    "    start_of_week = start_of_week.date()\n",
    "    end_of_week = start_of_week + datetime.timedelta(days=6)\n",
    "    \n",
    "    \n",
    "    return str(start_of_week) + \"--\" + str(end_of_week)\n",
    "\n",
    "df_counts_week['week_period'] = df_counts_week['week'].apply(week_period)\n",
    "import matplotlib.ticker as mtick\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "\n",
    "# Convert the week_rate column to a float\n",
    "df_counts_week = df_counts_week.tail(15)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5),gridspec_kw={'wspace': 0.6})\n",
    "\n",
    "axs[0].plot(df_counts_month['month'],df_counts_month['FileName'],marker='o')\n",
    "axs[0].set_xticks(df_counts_month['month'])\n",
    "axs[0].set_xticklabels(df_counts_month['month'],rotation=45)\n",
    "\n",
    "# y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "# axs[0].yaxis.set_major_formatter(y_fmt)\n",
    "\n",
    "positions = range(len(df_counts_month))\n",
    "bar_width = 0.2\n",
    "adj_positions = [pos + bar_width for pos in positions]\n",
    "axs0 = axs[0].twinx()\n",
    "# Plot the bar graph\n",
    "axs0.bar(positions, df_counts_month['FileName'], color='#6495ED', width=bar_width, label='Num_Camcom')\n",
    "#axs0.bar(adj_positions, month_rate['Num_MV'], color='blue', width=bar_width, label='Num_MV')\n",
    "axs0.set_ylabel('Number')\n",
    "# axs0.tick_params('y', colors='red')\n",
    "\n",
    "axs[0].set_xlabel('Month')\n",
    "axs[0].set_ylabel('Month Rate (as percentage)')\n",
    "axs[0].set_title('Month Rate vs Month')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axs[1].plot(df_counts_week['week_period'],df_counts_week['FileName'],marker='o')\n",
    "axs[1].set_xticks(df_counts_week['week_period'])\n",
    "axs[1].set_xticklabels(df_counts_week['week_period'],rotation=90)\n",
    "# y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "# axs[1].yaxis.set_major_formatter(y_fmt)\n",
    "positions = range(len(df_counts_week))\n",
    "bar_width = 0.2\n",
    "adj_positions = [pos + bar_width for pos in positions]\n",
    "axs1 = axs[1].twinx()\n",
    "# Plot the bar graph\n",
    "axs1.bar(positions, df_counts_week['FileName'], color='#6495ED', width=bar_width, label='Num_Camcom')\n",
    "#axs0.bar(adj_positions, month_rate['Num_MV'], color='blue', width=bar_width, label='Num_MV')\n",
    "axs1.set_ylabel('Number')\n",
    "# axs0.tick_params('y', colors='red')\n",
    "axs[1].set_xlabel('Week Period')\n",
    "axs[1].set_ylabel('Week Rate (as percentage)')\n",
    "axs[1].set_title('Week Rate vs Week Period')\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "fig.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "html_graph_camcom_usage = f'<img src=\"data:image/png;base64,{image_data}\">'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7e537b-69db-49c9-b0d6-0f82d7831190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a64f5f-cf1c-4390-b45b-b1d47e0421fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Graph for adoption rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dccac13-78f7-4a43-aaa7-42725119b282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "week_df['week'] = week_df['week'].apply(lambda x: str(x).zfill(2))\n",
    "\n",
    "current_week = datetime.datetime.now().strftime('%U')\n",
    "if current_week not in week_df['week'].tolist():\n",
    "    week_df = week_df.append({'week':str(current_week).zfill(2),'Num_Camcom':0,'Num_MV':0,'week_rate':\"0%\"},ignore_index=True)\n",
    "for i in range(5, int(current_week)):\n",
    "    if str(i).zfill(2) not in week_df['week'].tolist():\n",
    "        week_df = week_df.append({'week':str(i).zfill(2),'Num_Camcom':0,'Num_MV':0,'week_rate':\"0%\"},ignore_index=True)\n",
    "\n",
    "\n",
    "def week_period(week_number):\n",
    "    week_number = int(week_number)\n",
    "    year = 2023\n",
    "    first_day = datetime.datetime(year,1,1)\n",
    "    day_offset = (week_number -1)*7\n",
    "    start_of_week = first_day + datetime.timedelta(days=day_offset)\n",
    "    start_of_week = start_of_week.date()\n",
    "    end_of_week = start_of_week + datetime.timedelta(days=6)\n",
    "    \n",
    "    \n",
    "    return str(start_of_week) + \"--\" + str(end_of_week)\n",
    "\n",
    "week_df['week_period'] = week_df['week'].apply(week_period)\n",
    "week_df = week_df.sort_values(by='week')\n",
    "\n",
    "partner_week['week_period'] = partner_week['week'].apply(week_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8833177c-27c4-40e2-b365-bbdbca3b5ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "\n",
    "# Convert the week_rate column to a float\n",
    "week_df1 = week_df.tail(9)\n",
    "month_rate['month_rate'] = month_rate['month_rate'].str.rstrip('%').astype('float') / 100\n",
    "week_df1['week_rate'] = week_df1['week_rate'].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5),gridspec_kw={'wspace': 0.6})\n",
    "\n",
    "axs[0].plot(month_rate['month'],month_rate['month_rate'],marker='o')\n",
    "axs[0].set_xticks(month_rate['month'])\n",
    "axs[0].set_xticklabels(month_rate['month'],rotation=45)\n",
    "y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "axs[0].yaxis.set_major_formatter(y_fmt)\n",
    "\n",
    "positions = range(len(month_rate))\n",
    "bar_width = 0.2\n",
    "adj_positions = [pos + bar_width for pos in positions]\n",
    "axs0 = axs[0].twinx()\n",
    "# Plot the bar graph\n",
    "axs0.bar(positions, month_rate['Num_Camcom'], color='#6495ED', width=bar_width, label='Num_Camcom')\n",
    "#axs0.bar(adj_positions, month_rate['Num_MV'], color='blue', width=bar_width, label='Num_MV')\n",
    "axs0.set_ylabel('Number')\n",
    "# axs0.tick_params('y', colors='red')\n",
    "\n",
    "axs[0].set_xlabel('Month')\n",
    "axs[0].set_ylabel('Month Rate (as percentage)')\n",
    "axs[0].set_title('Month Rate vs Month')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axs[1].plot(week_df1['week_period'],week_df1['week_rate'],marker='o')\n",
    "axs[1].set_xticks(week_df1['week_period'])\n",
    "axs[1].set_xticklabels(week_df1['week_period'],rotation=90)\n",
    "y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "axs[1].yaxis.set_major_formatter(y_fmt)\n",
    "positions = range(len(week_df1))\n",
    "bar_width = 0.2\n",
    "adj_positions = [pos + bar_width for pos in positions]\n",
    "axs1 = axs[1].twinx()\n",
    "# Plot the bar graph\n",
    "\n",
    "axs1.bar(positions, week_df1['Num_Camcom'], color='#6495ED', width=bar_width, label='Num_Camcom')\n",
    "#axs0.bar(adj_positions, month_rate['Num_MV'], color='blue', width=bar_width, label='Num_MV')\n",
    "axs1.set_ylabel('Number')\n",
    "# axs0.tick_params('y', colors='red')\n",
    "axs[1].set_xlabel('Week Period')\n",
    "axs[1].set_ylabel('Week Rate (as percentage)')\n",
    "axs[1].set_title('Week Rate vs Week Period')\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "fig.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "html_graph = f'<img src=\"data:image/png;base64,{image_data}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e1118c-e11c-4b15-8f84-efc1c6745343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_partner_month['month_rate'] = df_partner_month['month_rate'].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "    \n",
    "for i, partner in enumerate(df_partner_month['Type'].unique()):\n",
    "    data = df_partner_month[df_partner_month['Type'] == partner]\n",
    "    if i == 0:\n",
    "        axs[0].plot(data['month'], data['month_rate'], color='blue', label=partner,marker='o')\n",
    "    else:\n",
    "        \n",
    "        axs[0].plot(data['month'], data['month_rate'], color='green', label=partner,marker='o')\n",
    "    # elif i == 2:\n",
    "    #     axs[0].plot(data['month'], data['month_rate'], color='orange', label=partner,marker='o')\n",
    "    # else:\n",
    "    #     axs[0].plot(data['month'], data['month_rate'], color='#8B5A2B', label=partner,marker='o') # Light Brown\n",
    "\n",
    "        \n",
    "# Add labels and legend\n",
    "axs[0].set_xlabel('month')\n",
    "axs[0].set_xticklabels(df_partner_month['month'].unique())\n",
    "#ax.set_xticklabels(week_df['week_period'],rotation=45)\n",
    "y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "axs[0].yaxis.set_major_formatter(y_fmt)\n",
    "axs[0].set_ylabel('Monthly Rate')\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "\n",
    "partner_week['week_rate'] = partner_week['week_rate'].str.rstrip('%').astype('float') / 100\n",
    "    \n",
    "for i, partner in enumerate(partner_week['Type'].unique()):\n",
    "    data = partner_week[partner_week['Type'] == partner]\n",
    "    if i == 0:\n",
    "        axs[1].plot(data['week_period'], data['week_rate'], color='blue', label=partner,marker='o')\n",
    "    else:\n",
    "        \n",
    "        axs[1].plot(data['week_period'], data['week_rate'], color='green', label=partner,marker='o')\n",
    "    # elif i == 2:\n",
    "    #     axs[1].plot(data['week_period'], data['week_rate'], color='orange', label=partner,marker='o')\n",
    "    # else:\n",
    "    #     axs[1].plot(data['week_period'], data['week_rate'], color='#8B5A2B', label=partner,marker='o') # Light Brown\n",
    "\n",
    "        \n",
    "# Add labels and legend\n",
    "axs[1].set_xlabel('week_period')\n",
    "axs[1].set_xticklabels(partner_week['week_period'].unique(),rotation=90)\n",
    "#ax.set_xticklabels(week_df['week_period'],rotation=45)\n",
    "y_fmt = mtick.PercentFormatter(xmax=1.0, decimals=2)\n",
    "axs[1].yaxis.set_major_formatter(y_fmt)\n",
    "axs[1].set_ylabel('Week Rate')\n",
    "axs[1].legend(loc='upper left')\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "fig.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "html_graph_channel = f'<img src=\"data:image/png;base64,{image_data}\">'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89277631-bac3-40e7-bd0a-b2c3d27e45e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Graph for surveyor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9d071c-7bbc-4bd6-b223-8c8413df2e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "survey_job_s = pd.read_csv(\"xxxx\"')\n",
    "survey_job_a = pd.read_csv(\"xxxx\")\n",
    "survey_job_a = survey_job_a[~survey_job_a['status'].isna()].reset_index().copy()\n",
    "survey_job_a = survey_job_a[survey_job_a['status'] == 'review_completed']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fc932a-c034-4e41-9347-fd7032a73022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_unqualified_images_a = survey_job_a[['License Plate','Inspector','No of Failed Images','week','month']].copy()\n",
    "avg_unqualified_images_a = avg_unqualified_images_a[avg_unqualified_images_a['week']>=5]\n",
    "week_avg_unqualified_images_a = avg_unqualified_images_a[['Inspector','week','No of Failed Images']].groupby(['Inspector','week']).mean().reset_index()\n",
    "week_avg_unqualified_images_a = week_avg_unqualified_images_a.ffill()\n",
    "\n",
    "\n",
    "current_week = datetime.datetime.now().strftime('%U')\n",
    "week_avg_unqualified_images_1= pd.DataFrame()\n",
    "for i, inspector in enumerate(week_avg_unqualified_images_a['Inspector'].unique()):\n",
    "\n",
    "    data = week_avg_unqualified_images_a[week_avg_unqualified_images_a['Inspector'] == inspector]\n",
    "\n",
    "    if int(current_week) not in data['week'].tolist():\n",
    "        data=data.append({'week':str(current_week).zfill(2),'No of Failed Images':0,'Inspector':inspector},ignore_index=True)\n",
    "\n",
    "    for i in range(5, int(current_week)):\n",
    "        if i not in data['week'].tolist():\n",
    "            data = data.append({'week':str(i).zfill(2),'No of Failed Images':0,'Inspector':inspector},ignore_index=True)\n",
    "            data.sort_values(by=['Inspector','week'],inplace=True)\n",
    "    \n",
    "        \n",
    "    week_avg_unqualified_images_1 = pd.concat([week_avg_unqualified_images_1,data])\n",
    "week_avg_unqualified_images_1['week'] = week_avg_unqualified_images_1['week'].astype(int)\n",
    "week_avg_unqualified_images_1 = week_avg_unqualified_images_1.sort_values(by=['Inspector','week'])\n",
    "week_avg_unqualified_images_1['week_period'] = week_avg_unqualified_images_1['week'].apply(week_period)\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "week_avg_unqualified_images_1 = week_avg_unqualified_images_1[week_avg_unqualified_images_1['week'].astype(int) >= 5]\n",
    "    \n",
    "for i, inspector in enumerate(week_avg_unqualified_images_1['Inspector'].unique()):\n",
    "    data = week_avg_unqualified_images_1[week_avg_unqualified_images_1['Inspector'] == inspector]\n",
    "    if i == 0:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='blue', label=inspector,marker='o')\n",
    "    elif i == 1:\n",
    "        data.sort_values(by='week_period',inplace=True)\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='green', label=inspector,marker='o')\n",
    "\n",
    "    elif i == 2:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='orange', label=inspector,marker='o')\n",
    "    else:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='#8B5A2B', label=inspector,marker='o') # Light Brown\n",
    "\n",
    "\n",
    "        \n",
    "# Add labels and legend\n",
    "\n",
    "ax.set_xlabel('week_period')\n",
    "ax.set_xticklabels(week_avg_unqualified_images_1[week_avg_unqualified_images_1['Inspector'] == 'Inspector 2 - Edi Kartono - 082198508597']['week_period'],rotation=90)\n",
    "#ax.set_xticklabels(week_df['week_period'],rotation=45)\n",
    "ax.set_ylabel('Avg week num of unqualified images')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Ali Team Surveyors Performance')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "fig.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "html_graph_unqualified_ali = f'<img src=\"data:image/png;base64,{image_data}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818998b2-4bda-4dad-828e-f68880003622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_unqualified_images_shita = survey_job_shita[['License Plate','Inspector','No of Failed Images','week','month']].copy()\n",
    "avg_unqualified_images_shita = avg_unqualified_images_shita[avg_unqualified_images_shita['week']>=23]\n",
    "\n",
    "grouped = avg_unqualified_images_shita[['Inspector','No of Failed Images']].groupby('Inspector')['No of Failed Images'].mean().sort_values(ascending=False).reset_index()\n",
    "top_inspectors = grouped.head(4)\n",
    "avg_unqualified_images_shita = avg_unqualified_images_shita[avg_unqualified_images_shita['Inspector'].isin(list(top_inspectors['Inspector']))]\n",
    "\n",
    "week_avg_unqualified_images_shita = avg_unqualified_images_shita[['Inspector','week','No of Failed Images']].groupby(['Inspector','week']).mean().reset_index()\n",
    "week_avg_unqualified_images_shita = week_avg_unqualified_images_shita.ffill()\n",
    "\n",
    "\n",
    "current_week = datetime.datetime.now().strftime('%U')\n",
    "week_avg_unqualified_images_1= pd.DataFrame()\n",
    "for i, inspector in enumerate(week_avg_unqualified_images_shita['Inspector'].unique()):\n",
    "\n",
    "    data = week_avg_unqualified_images_shita[week_avg_unqualified_images_shita['Inspector'] == inspector]\n",
    "\n",
    "    if int(current_week) not in data['week'].tolist():\n",
    "        data=data.append({'week':str(current_week).zfill(2),'No of Failed Images':0,'Inspector':inspector},ignore_index=True)\n",
    "\n",
    "    for i in range(23, int(current_week)):\n",
    "        if i not in data['week'].tolist():\n",
    "            data = data.append({'week':str(i).zfill(2),'No of Failed Images':0,'Inspector':inspector},ignore_index=True)\n",
    "            data.sort_values(by=['Inspector','week'],inplace=True)\n",
    "            print(data)\n",
    "    \n",
    "        \n",
    "    week_avg_unqualified_images_1 = pd.concat([week_avg_unqualified_images_1,data])\n",
    "week_avg_unqualified_images_1['week'] = week_avg_unqualified_images_1['week'].astype(int)\n",
    "week_avg_unqualified_images_1 = week_avg_unqualified_images_1.sort_values(by=['Inspector','week'])\n",
    "week_avg_unqualified_images_1['week_period'] = week_avg_unqualified_images_1['week'].apply(week_period)\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "week_avg_unqualified_images_1 = week_avg_unqualified_images_1[week_avg_unqualified_images_1['week'].astype(int) >= 23]\n",
    "    \n",
    "for i, inspector in enumerate(week_avg_unqualified_images_1['Inspector'].unique()):\n",
    "    data = week_avg_unqualified_images_1[week_avg_unqualified_images_1['Inspector'] == inspector]\n",
    "    if i == 0:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='blue', label=inspector,marker='o')\n",
    "    elif i == 1:\n",
    "        data.sort_values(by='week_period',inplace=True)\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='green', label=inspector,marker='o')\n",
    "\n",
    "    elif i == 2:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='orange', label=inspector,marker='o')\n",
    "    else:\n",
    "        ax.plot(data['week_period'], data['No of Failed Images'], color='#8B5A2B', label=inspector,marker='o') # Light Brown\n",
    "\n",
    "\n",
    "        \n",
    "# Add labels and legend\n",
    "\n",
    "ax.set_xlabel('week_period')\n",
    "ax.set_xticklabels(week_avg_unqualified_images_1[week_avg_unqualified_images_1['Inspector'] == list(top_inspectors['Inspector'])[0]]['week_period'],rotation=90)\n",
    "#ax.set_xticklabels(week_df['week_period'],rotation=45)\n",
    "ax.set_ylabel('Avg week num of unqualified images')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Shita Team Surveyors Performance')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "fig.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "html_graph_unqualified_shita = f'<img src=\"data:image/png;base64,{image_data}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da99d095-e617-4779-bf9c-65b404ed1cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message_1 = '''\n",
    "<head>\n",
    "    <title>Error KPIs CamCom</title>\n",
    "    <style>\n",
    "      table,\n",
    "      th,\n",
    "      td {{\n",
    "        padding: 10px;\n",
    "        border: 1px solid black;\n",
    "        border-collapse: collapse;\n",
    "      }}\n",
    "    </style>\n",
    "</head>\n",
    "'''\n",
    "end_txt = '''<p> For further details on unqualified images and missed sides, please view attachment </p>'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363ac5d0-2ce8-4c5a-8cbf-3b456ffc4c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert the dataframe to html\n",
    "# html_task = task_to_check.to_html()\n",
    "# task_to_check = '''\n",
    "# <p>This is the tasks we need to follow up, please check the following table:</p><br>\n",
    "# '''\n",
    "\n",
    "html_table1 = Adoption_Rate.to_html()\n",
    "html_table5 = month_df.to_html()\n",
    "html_table4 = week_rate.to_html()\n",
    "new_df_html = new_df.to_html()\n",
    "\n",
    "## zero_rate\n",
    "Start_text = '''\n",
    "<p>Hi Team:</p>\n",
    "'''\n",
    "Start_text_1 = '''\n",
    "<p>An update for the adoption rate and surveyers perfotmance. Please review following tables and graphs.</p>\n",
    "'''\n",
    "\n",
    "zero_rate_text = '''\n",
    "<p>This is the total rate for the TAT < 1day :</p><br>\n",
    "'''\n",
    "zero_rate = \"{:.2f}%\".format(zero_rate * 100)\n",
    "zero_rate = \"<p>The formatted percentage is: <strong>{}</strong></p>\".format(zero_rate)\n",
    "zero_rate_month_text = '''\n",
    "<p>This is the monthly rate for the TAT<1day:</p><br>\n",
    "'''\n",
    "\n",
    "\n",
    "additional_text_1 = '''\n",
    "<p>This is the monthly adoption rate graph (only track agent&partner at this stage):</p><br>\n",
    "The calculation of adoption rate: b/a<br>\n",
    "a = Total number of USED cars in MV_Portal Report from Feb 2023<br>\n",
    "b = Total number of cases can be found in Camcom Protal<br>\n",
    "'''\n",
    "additional_text_2 = '''\n",
    "<p>This is the surveyors performance with the number of failed cases and also the average weekly unqualified images:</p><br>\n",
    "'''\n",
    "additional_text_3 = '''\n",
    "<br>\n",
    "'''\n",
    "additional_text_4 = '''\n",
    "<p>This is the weekly adoption rate:</p><br>\n",
    "'''\n",
    "additional_text_5 = '''\n",
    "<p>For more details,pls refer to the table below:</p><br>\n",
    "'''\n",
    "additional_text_6 = '''\n",
    "<p>Following graph is the usage of CamCom.</p><br>'''\n",
    "\n",
    "# final_html = Start_text+Start_text_1 + additional_text_1 + html_graph + html_graph_channel+ additional_text_5 +html_table5  + additional_text_4 + html_table4  + zero_rate_text+zero_rate +zero_rate_month_text+new_df_html+additional_text_2+ html_table2 +html_graph_unqualified + end_txt\n",
    "\n",
    "final_html = Start_text+Start_text_1 + additional_text_1 + html_graph + html_graph_channel+additional_text_6+html_graph_camcom_usage +additional_text_5 +html_table5  + additional_text_4 + html_table4  + zero_rate_text+zero_rate +zero_rate_month_text+new_df_html +additional_text_2+ html_graph_unqualified_ali +html_graph_unqualified_shita + end_txt\n",
    "\n",
    "\n",
    "#final_html = task_to_check + html_task\n",
    "\n",
    "# print the final HTML code\n",
    "print(final_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380c431a-b229-47e0-b1e2-07b42d74d283",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_file = open(\"/dbfs/mnt/idpdmccpstrappdataz01/errors/sample.txt\", \"w\")\n",
    "n = text_file.write(final_html)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29cc8f00-d5b8-43f3-a67e-33fd65f0f6dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27408e4-cbc2-4685-9286-7b340737c23f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CamCom.Failed.KPIs_export",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
